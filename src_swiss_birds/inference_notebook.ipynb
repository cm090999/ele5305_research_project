{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE UNLESS RE-GENERATING DATASET IMAGES ###\n",
    "class Config_Mel():\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        # Device\n",
    "        self.device = 'cpu'\n",
    "        \n",
    "        # Dataset Path\n",
    "        self.birdclef2023 = 'birdclef-2023'\n",
    "\n",
    "        # Out path\n",
    "        self.outpath_images = self.birdclef2023 + '_MelSpectrograms'\n",
    "\n",
    "        self.melSpecTransform = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        # Audio Features\n",
    "        self.sample_rate = 32000\n",
    "        self.n_fft=2048\n",
    "        self.f_min=40\n",
    "        self.f_max=15000\n",
    "        self.hop_length=512\n",
    "        self.n_mels=128\n",
    "        self.mel_args = {'sample_rate': self.sample_rate,\n",
    "                         'n_fft': self.n_fft,\n",
    "                         'f_min': self.f_min,\n",
    "                         'f_max': self.f_max,\n",
    "                         'hop_length': self.hop_length,\n",
    "                         'n_mels': self.n_mels}\n",
    "### DO NOT CHANGE UNLESS RE-GENERATING DATASET IMAGES ###\n",
    "\n",
    "class Config():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.run_environment = 'local' # 'kaggle' 'local' 'colab'\n",
    "\n",
    "        # Device\n",
    "        if (self.run_environment == 'kaggle') or (self.run_environment == 'colab'):\n",
    "            self.device = 'cpu'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        # Dataset Path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.soundscape_paths = \"/kaggle/input/birdclef-2023/test_soundscapes\"\n",
    "        elif self.run_environment == 'local':\n",
    "            self.soundscape_paths = '/home/colin/elec5305/ele5305_research_project/birdclef-2023/test_soundscapes'\n",
    "        elif self.run_environment == 'colab':\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # Metadata Path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.metadata_path = '/kaggle/input/birdclef-2023/train_metadata.csv'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.metadata_path = '/home/colin/elec5305/ele5305_research_project/birdclef-2023/train_metadata.csv'\n",
    "        elif self.run_environment == 'colab':\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Out path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.outpath = '/kaggle/working/'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.outpath = '/home/colin/elec5305/ele5305_research_project/src_mel/results'\n",
    "        elif self.run_environment == 'colab':\n",
    "            self.outpath = ''\n",
    "\n",
    "        # Dataloader options\n",
    "        self.num_workers = 2\n",
    "        self.test_batch_size = 64\n",
    "\n",
    "        # Model name\n",
    "        self.model_name = 'tf_efficientnet_b0_ns'\n",
    "\n",
    "        # Weights path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.pretrained_weights = '/kaggle/input/weigths/model_weights.pth'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.pretrained_weights = '/home/colin/elec5305/ele5305_research_project/src_audio/weights/model_weights.pth'\n",
    "\n",
    "        # Image Transforms\n",
    "        self.test_transforms = torchvision.transforms.Compose([\n",
    "                    # transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "        # Audio Features\n",
    "        self.sample_rate = 32000\n",
    "        self.period = 5\n",
    "\n",
    "        # Mel Spectrogram Parameters\n",
    "        self.n_fft=2048\n",
    "        self.f_min=40\n",
    "        self.f_max=15000\n",
    "        self.hop_length=512\n",
    "        self.n_mels=128\n",
    "        self.mel_args = {'n_fft': self.n_fft,\n",
    "                         'f_min': self.f_min,\n",
    "                         'f_max': self.f_max,\n",
    "                         'hop_length': self.hop_length,\n",
    "                         'n_mels': self.n_mels}\n",
    "        \n",
    "CONFIG = Config()\n",
    "CONFIG_MEL = Config_Mel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(CONFIG.soundscape_paths).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"name\" ,\"id\", \"path\"]\n",
    ")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEF2023_SoundScapes(torch.utils.data.Dataset):\n",
    "    def __init__(self, test_df, sample_rate, period, transform):\n",
    "        self.test_df = test_df\n",
    "        self.sample_rate = sample_rate\n",
    "        self.period = period\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.test_df['id']))\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        dict_idx = dict(self.test_df.iloc[idx])\n",
    "\n",
    "        ogg_file = dict_idx['path']\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=self.sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "        recording_length = len(waveform) / self.sample_rate\n",
    "        n_periods = int(recording_length // self.period)\n",
    "\n",
    "\n",
    "        waveform_list = []\n",
    "        for i in range(n_periods):\n",
    "            curr_waveform = waveform[i * self.sample_rate * self.period : (i+1) * self.sample_rate * self.period]\n",
    "\n",
    "            # Append the current tensor to the list\n",
    "            waveform_list.append(curr_waveform)\n",
    "\n",
    "        # Stack the tensors in the list along the 0th dimension\n",
    "        waveforms = torch.stack(waveform_list, dim=0)\n",
    "        dict_idx['waveforms'] = waveforms\n",
    "\n",
    "        return dict_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test dataset instance\n",
    "test_dataset = BirdCLEF2023_SoundScapes(test_df=df_test, sample_rate=CONFIG.sample_rate, period = CONFIG.period, transform=CONFIG.test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "print(test_dataset[0]['waveforms'].shape)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramLayer(nn.Module):\n",
    "    def __init__(self, sample_rate, n_fft, hop_length, n_mels, transform):\n",
    "        super(MelSpectrogramLayer, self).__init__()\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        mel_spectrogram = self.mel_transform(waveform)\n",
    "\n",
    "        batched = True\n",
    "        if mel_spectrogram.dim() == 2:\n",
    "            batched = False\n",
    "        elif mel_spectrogram.dim() == 3:\n",
    "            batched = True\n",
    "\n",
    "        if batched == True:\n",
    "            if self.training and torch.rand(1) >= CONFIG.masking_prob and CONFIG.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[2] // 5\n",
    "                )(mel_spectrogram)\n",
    "        else:\n",
    "            if self.training and torch.rand(1) >= CONFIG.masking_prob and CONFIG.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[0] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "\n",
    "\n",
    "        if batched == True:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(1)\n",
    "            mel_spectrogram = mel_spectrogram.expand(-1, 3, -1, -1)\n",
    "        else:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(0)\n",
    "            mel_spectrogram = mel_spectrogram.expand(3, -1, -1)\n",
    "\n",
    "        mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        mel_spectrogram = torch.nan_to_num(mel_spectrogram)\n",
    "        \n",
    "        return mel_spectrogram\n",
    "    \n",
    "\n",
    "# https://www.kaggle.com/code/leonshangguan/faster-eb0-sed-model-inference\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = torch.nn.functional.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class Mel_Classifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        mel_generator: MelSpectrogramLayer,\n",
    "        # config=None,\n",
    "        pretrained=True, \n",
    "        num_classes=264, \n",
    "        in_channels=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.config = config\n",
    "\n",
    "        self.mel_generator = mel_generator\n",
    "\n",
    "        # self.bn0 = nn.BatchNorm2d(self.config.n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=0,\n",
    "            global_pool=\"\",\n",
    "            in_chans=in_channels,\n",
    "        )\n",
    "        \n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"linear\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        # init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        input_data = self.mel_generator(input_data)\n",
    "\n",
    "        # if self.config.in_channels == 3:\n",
    "        x = input_data\n",
    "        # else:\n",
    "        #     x = input_data[:, [0], :, :] # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        # frames_num = x.shape[2]\n",
    "\n",
    "        # x = x.transpose(1, 3)\n",
    "        # x = self.bn0(x)\n",
    "        # x = x.transpose(1, 3)\n",
    "\n",
    "\n",
    "        # x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = torch.nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = torch.nn.functional.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.nn.functional.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "        output_dict = {\n",
    "            \"clipwise_output\": clipwise_output,\n",
    "        }\n",
    "\n",
    "        # return output_dict\n",
    "        return clipwise_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "melspec_layer = MelSpectrogramLayer(sample_rate=CONFIG.sample_rate,\n",
    "                                    n_fft=CONFIG.n_fft,\n",
    "                                    hop_length=CONFIG.hop_length,\n",
    "                                    n_mels=CONFIG.n_mels,\n",
    "                                    transform=CONFIG.test_transforms)\n",
    "network = Mel_Classifier(model_name=CONFIG.model_name,\n",
    "                         mel_generator=melspec_layer,\n",
    "                         pretrained=False)\n",
    "\n",
    "# Load weights\n",
    "network.load_state_dict(torch.load(CONFIG.pretrained_weights, map_location=CONFIG.device))\n",
    "\n",
    "# Set to eval\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def padded_cmap(solution, submission, padding_factor=5):\n",
    "    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n",
    "    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n",
    "    new_rows = []\n",
    "    for i in range(padding_factor):\n",
    "        new_rows.append([1 for i in range(len(solution.columns))])\n",
    "    new_rows = pd.DataFrame(new_rows)\n",
    "    new_rows.columns = solution.columns\n",
    "    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n",
    "    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n",
    "    score = sklearn.metrics.average_precision_score(\n",
    "        padded_solution.values,\n",
    "        padded_submission.values,\n",
    "        average='macro',\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get species list\n",
    "df_all = pd.read_csv(CONFIG.metadata_path)\n",
    "species_list = list(set(df_all['primary_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toProbs = torch.nn.Softmax(dim=1)\n",
    "\n",
    "outputs = [0] * len(test_dataset)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    pred = toProbs(network(test_dataset[i]['waveforms'])).detach().cpu().numpy()\n",
    "\n",
    "    outputs[i] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = df_test.filename.values.tolist()\n",
    "sub_df = pd.DataFrame(columns=['row_id']+species_list)\n",
    "\n",
    "for i, file in enumerate(filenames):\n",
    "    pred = outputs[i]\n",
    "    num_rows = pred.shape[0]\n",
    "\n",
    "    row_ids = [f'{file}_{(i+1)*5}' for i in range(num_rows)]\n",
    "    df = pd.DataFrame(columns=['row_id']+species_list)\n",
    "    \n",
    "    df['row_id'] = row_ids\n",
    "\n",
    "\n",
    "    df[species_list] = pred\n",
    "\n",
    "    sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(CONFIG.outpath, 'submission.csv')\n",
    "sub_df.to_csv(csv_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
