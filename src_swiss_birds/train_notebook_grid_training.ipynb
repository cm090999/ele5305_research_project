{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook for BirdCLEF2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all Dependencies (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from tqdm import tqdm\n",
    "import audiomentations as AA\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import bisect\n",
    "import json\n",
    "import shutil\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CONFIG class containing all relevant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.run_environment = 'local' # 'kaggle' 'local' 'colab'\n",
    "        self.load_pretrained_weights = False\n",
    "        self.training_data_per_epoch = 1\n",
    "        self.soft_second_label = 0.3\n",
    "        self.class_weighting = False\n",
    "        self.softmax_prob = True\n",
    "        self.use_mixup = True\n",
    "        self.mix_up_prob = 0.5\n",
    "        self.criterion = 'LabelSmoothingBCEWithLogitsLoss' # 'BCEFocalLoss' # 'LabelSmoothingBCEWithLogitsLoss' # 'LabelSmoothingBCEWithLogitsLoss' # 'CrossEntropyLoss' # 'BCEWithLogitsLoss'\n",
    "        self.epochs = 30\n",
    "        self.masking = False\n",
    "        self.masking_prob = 0.005\n",
    "        self.frac_nocall = 0.01\n",
    "        self.use_nocall = False\n",
    "        self.addbackground_prob = 0.0\n",
    "        self.uniform_sampler = False\n",
    "        self.with_replacement_valid = False\n",
    "        self.with_replacement_train = False\n",
    "        self.analyze_dataset = False\n",
    "        self.fix_features = False\n",
    "        self.augment_audio_train = True\n",
    "\n",
    "        # Device\n",
    "        if (self.run_environment == 'kaggle') or (self.run_environment == 'colab'):\n",
    "            self.device = 'cpu'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        # Dataset Path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.dataset_swiss_birds_train = '/kaggle/input/swissbirds/swiss_birds/wav_files/train'\n",
    "            self.dataset_swiss_birds_training_metadata = '/kaggle/input/swissbirds/swiss_birds/metadata_train.csv'\n",
    "            self.birdclef2021_background_noise = '/kaggle/input/birdclef2021-background-noise/ff1010bird_nocall/nocall' \n",
    "            self.birdclef2021_background_noise_metadata = '/kaggle/input/birdclef2021-background-noise/ff1010bird_nocall/ff1010bird_metadata_v1.csv'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.dataset_swiss_birds_train = '/home/colin/elec5305/ele5305_research_project/swiss_birds/wav_files/train'\n",
    "            self.dataset_swiss_birds_training_metadata = '/home/colin/elec5305/ele5305_research_project/swiss_birds/metadata_train.csv'\n",
    "            self.birdclef2021_background_noise = '/home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall/nocall' \n",
    "            self.birdclef2021_background_noise_metadata = '/home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall/ff1010bird_metadata_v1.csv'\n",
    "        elif self.run_environment == 'colab':\n",
    "            self.birdclef2023_melspectrograms = ''\n",
    "\n",
    "        # Out path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.outpath = '/kaggle/working/results'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.outpath = 'results'\n",
    "        elif self.run_environment == 'colab':\n",
    "            self.outpath = ''\n",
    "            \n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.back1 = '/kaggle/input/birdclef2021-background-noise/ff1010bird_nocall/nocall'\n",
    "            self.back2 = '/kaggle/input/birdclef2021-background-noise/train_soundscapes/nocall'\n",
    "            self.back3 = '/kaggle/input/birdclef2021-background-noise/aicrowd2020_noise_30sec/noise_30sec'\n",
    "        else:\n",
    "            self.back1 = \"/home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall/nocall\"\n",
    "            self.back2 = \"/home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/train_soundscapes/nocall\"\n",
    "            self.back3 = \"/home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/aicrowd2020_noise_30sec/noise_30sec\"\n",
    "\n",
    "        # Dataset Path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.dataset_swiss_birds_test = '/kaggle/input/swissbirds/swiss_birds/wav_files/test'\n",
    "            self.dataset_swiss_birds_test_metadata = '/kaggle/input/swissbirds/swiss_birds/metadata_test.csv'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.dataset_swiss_birds_test = '/home/colin/elec5305/ele5305_research_project/swiss_birds/wav_files/test'\n",
    "            self.dataset_swiss_birds_test_metadata = '/home/colin/elec5305/ele5305_research_project/swiss_birds/metadata_test.csv'\n",
    " \n",
    "        # Train/Validation Split \n",
    "        self.val_frac = 0.1\n",
    "\n",
    "        # Dataloader options\n",
    "        self.num_workers = 4\n",
    "        self.train_batch_size = 32\n",
    "        self.valid_batch_size = 32\n",
    "        self.test_batch_size = 4\n",
    "\n",
    "        # Model name\n",
    "        self.model_name = 'tf_efficientnet_b0.ns_jft_in1k'\n",
    "        # Pretrained\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.pretrained_weights = '/kaggle/input/2023-11-09-14-03-57/model_weights.pth'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.pretrained_weights = '/home/colin/elec5305/ele5305_research_project/weights/model_weights.pth'\n",
    "\n",
    "\n",
    "        # Optimizer Settings\n",
    "        self.lr=3e-4\n",
    "        self.weight_decay = 1e-3\n",
    "        self.momentum=0.9\n",
    "        self.optimizer = 'adam' # 'adam', 'sgd'\n",
    "\n",
    "        self.scheduler = 'cosineannealing'\n",
    "        self.eta_min = 1e-6\n",
    "        self.T_mult = 1\n",
    "        self.last_epoch = -1\n",
    "\n",
    "        self.mixup_alpha = 0.5\n",
    "\n",
    "        # Training Settings\n",
    "        self.print_every_n_batches = 25\n",
    "        self.patience = 100\n",
    "\n",
    "        # Image Transforms\n",
    "        self.train_transforms = torchvision.transforms.Compose([\n",
    "                    torchaudio.transforms.AmplitudeToDB(),\n",
    "                    # torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.RandomResizedCrop(size=(80,78), scale = (0.75, 1.0), antialias=True), \n",
    "                    ])\n",
    "        \n",
    "        self.val_transforms = torchvision.transforms.Compose([\n",
    "                    torchaudio.transforms.AmplitudeToDB(),\n",
    "                    # torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.Resize(size=[80,78], antialias=True),\n",
    "                    # torchvision.transforms.RandomResizedCrop(size=(128, 312), scale = (0.75, 1.0), antialias=True), \n",
    "                    ])\n",
    "        \n",
    "        self.test_transforms = torchvision.transforms.Compose([\n",
    "                    torchaudio.transforms.AmplitudeToDB(),\n",
    "                    torchvision.transforms.Resize(size=[80,78], antialias=True),\n",
    "                    ])\n",
    "        \n",
    "        # https://github.com/AtsunoriFujita/BirdCLEF-2023-Identify-bird-calls-in-soundscapes/blob/main/src/train_net.py\n",
    "        if self.augment_audio_train == True:\n",
    "            self.train_transforms_audio = AA.Compose(\n",
    "                                            [\n",
    "                                                AA.OneOf([\n",
    "                                                    AA.Gain(min_gain_in_db=-15, max_gain_in_db=15, p=1.0),\n",
    "                                                    AA.GainTransition(\n",
    "                                                        min_gain_in_db=-24.0,\n",
    "                                                        max_gain_in_db=6.0,\n",
    "                                                        min_duration=0.2,\n",
    "                                                        max_duration=6.0,\n",
    "                                                        p=1.0\n",
    "                                                    )\n",
    "                                                ], p=0.5,),\n",
    "                                                AA.OneOf([\n",
    "                                                    AA.AddGaussianNoise(p=1.0),\n",
    "                                                    AA.AddGaussianSNR(p=1.0),\n",
    "                                                ], p=0.3,),\n",
    "                                                AA.OneOf([\n",
    "                                                    AA.AddShortNoises(\n",
    "                                                        sounds_path=self.back1,\n",
    "                                                        min_snr_in_db=0,\n",
    "                                                        max_snr_in_db=3,\n",
    "                                                        p=1.0,\n",
    "                                                        lru_cache_size=10,\n",
    "                                                        min_time_between_sounds=4.0,\n",
    "                                                        max_time_between_sounds=16.0,\n",
    "                                                    ),\n",
    "                                                ], p=0.5,),\n",
    "                                                AA.OneOf([\n",
    "                                                    AA.AddBackgroundNoise(\n",
    "                                                        sounds_path=self.back2,\n",
    "                                                        min_snr_in_db=0,\n",
    "                                                        max_snr_in_db=3,\n",
    "                                                        p=1.0,\n",
    "                                                        lru_cache_size=3,),\n",
    "                                                    AA.AddBackgroundNoise(\n",
    "                                                        sounds_path=self.back3,\n",
    "                                                        min_snr_in_db=0,\n",
    "                                                        max_snr_in_db=3,\n",
    "                                                        p=1.0,\n",
    "                                                        lru_cache_size=450,),\n",
    "                                                ], p=0.5,),\n",
    "                                                AA.LowPassFilter(p=0.5),\n",
    "                                            ]\n",
    "                                        )\n",
    "        else:\n",
    "            self.train_transforms_audio = None\n",
    "                                            \n",
    "        self.val_transforms_audio = None\n",
    "        \n",
    "        self.test_transforms_audio = None\n",
    "\n",
    "\n",
    "        # Audio Features\n",
    "        self.sample_rate = 32000\n",
    "        self.period = 5\n",
    "\n",
    "        # Mel Spectrogram Parameters\n",
    "        self.n_fft=2048\n",
    "        self.f_min=40\n",
    "        self.f_max=15000\n",
    "        self.hop_length=1024\n",
    "        self.n_mels=80\n",
    "        self.mel_args = {'n_fft': self.n_fft,\n",
    "                         'f_min': self.f_min,\n",
    "                         'f_max': self.f_max,\n",
    "                         'hop_length': self.hop_length,\n",
    "                         'n_mels': self.n_mels}\n",
    "        \n",
    "        \n",
    "        if self.run_environment == 'colab':\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "\n",
    "            notebook_path = 'My Drive/elec5305'\n",
    "\n",
    "            env_path = f'/content/drive/{notebook_path}'\n",
    "            #Â Add the handout folder to python paths\n",
    "            if env_path not in sys.path:\n",
    "                sys.path.append(env_path)\n",
    "\n",
    "            # zip_path = os.path.join(env_path, 'birdclef-2023_MelSpectrograms.zip')\n",
    "            zip_path = '/content/drive/MyDrive/elec5305/birdclef-2023_MelSpectrograms.zip'\n",
    "            shutil.unpack_archive(zip_path, \"content/\")\n",
    "            print(zip_path)\n",
    "            # !unzip zip_path -d \"/content\"\n",
    "\n",
    "            # Dataset path\n",
    "            self.birdclef2023_melspectrograms = '/content/content/birdclef-2023_MelSpectrograms'\n",
    "\n",
    "            # Output path\n",
    "            self.outpath = os.path.join(env_path, 'results')\n",
    "            os.makedirs(self.outpath, exist_ok=True)# !pip install --force-reinstall numpy==1.22.1\n",
    "            \n",
    "            %pip install -q torchtoolbox timm\n",
    "\n",
    "            %pip install timm torchtoolbox\n",
    "\n",
    "        if self.run_environment != 'local':\n",
    "            # !pip install --force-reinstall numpy==1.22.1\n",
    "            %pip install -q torchtoolbox timm\n",
    "            \n",
    "        self.species_list = ['Song Thrush', 'Common Chaffinch', 'European Robin', 'Yellowhammer', 'Eurasian Wren', 'Common Nightingale', \"Western Bonelli's Warbler\", 'Common Redstart', 'Red Crossbill', 'Garden Warbler', 'Black Redstart', 'Tree Pipit', 'Marsh Warbler', 'Great Spotted Woodpecker', 'Tawny Owl', 'Common Blackbird', 'Willow Tit', 'Eurasian Eagle-Owl', 'Great Tit', 'Eurasian Blackcap', 'Common Chiffchaff', 'Eurasian Coot', 'Eurasian Reed Warbler', 'Yellow-legged Gull', 'Coal Tit']\n",
    "        \n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all dependencies (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtoolbox.tools import mixup_data, mixup_criterion\n",
    "from torch.nn.functional import cross_entropy\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramLayer(nn.Module):\n",
    "    def __init__(self, sample_rate, n_fft, hop_length, n_mels, transform, config):\n",
    "        super(MelSpectrogramLayer, self).__init__()\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, waveform):\n",
    "\n",
    "        mel_spectrogram = self.mel_transform(waveform)\n",
    "\n",
    "        batched = True\n",
    "        if mel_spectrogram.dim() == 2:\n",
    "            batched = False\n",
    "        elif mel_spectrogram.dim() == 3:\n",
    "            batched = True\n",
    "\n",
    "        if batched == True:\n",
    "            if self.training and torch.rand(1) >= self.config.masking_prob and self.config.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[2] // 5\n",
    "                )(mel_spectrogram)\n",
    "        else:\n",
    "            if self.training and torch.rand(1) >= self.config.masking_prob and self.config.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[0] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "\n",
    "\n",
    "        if batched == True:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(1)\n",
    "            mel_spectrogram = mel_spectrogram.expand(-1, 3, -1, -1)\n",
    "        else:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(0)\n",
    "            mel_spectrogram = mel_spectrogram.expand(3, -1, -1)\n",
    "\n",
    "        mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        mel_spectrogram = torch.nan_to_num(mel_spectrogram)\n",
    "\n",
    "        mel_spectrogram = self.normalize(mel_spectrogram)\n",
    "        \n",
    "        return mel_spectrogram\n",
    "    \n",
    "    def normalize(self, image):\n",
    "        mel_spectrogram = (image - image.min()) / ( image.max() - image.min())\n",
    "        return mel_spectrogram\n",
    "\n",
    "\n",
    "#####################################3\n",
    "    \n",
    "\n",
    "# https://www.kaggle.com/code/leonshangguan/faster-eb0-sed-model-inference\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class Mel_Classifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        mel_generator: MelSpectrogramLayer,\n",
    "        # config=None,\n",
    "        pretrained=True, \n",
    "        num_classes=25, \n",
    "        in_channels=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mel_generator = mel_generator\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=0,\n",
    "            global_pool=\"\",\n",
    "            in_chans=in_channels,\n",
    "        )\n",
    "        \n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"linear\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        # init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mel_generator(x)\n",
    "\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = torch.nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = torch.nn.functional.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.nn.functional.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "        # return output_dict\n",
    "        return clipwise_output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwissBirds Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwissBirds(torch.nn.Module):\n",
    "    def __init__(self, datapath: list, metadata_df, audio_transforms, sample_rate, config, period, soft_second_label, inherited_species_list = None, backgroundData = None, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Default values\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.period = period\n",
    "        self.config = config\n",
    "\n",
    "        self.backgroundData = backgroundData\n",
    "\n",
    "        self.df = metadata_df\n",
    "        self.datapath = datapath\n",
    "\n",
    "        self.soft_second_label = soft_second_label\n",
    "\n",
    "        if len(self.datapath) == 2:\n",
    "            self.audio_paths = [self.datapath[0], self.datapath[1]]\n",
    "        else:\n",
    "            self.audio_paths = [self.datapath[0]]\n",
    "\n",
    "        # Get species list\n",
    "        if inherited_species_list is None:\n",
    "            self.species = list(set(self.df['en']))\n",
    "        else:\n",
    "            self.species = inherited_species_list\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        # length = self.df['cumulative_images'][-1]\n",
    "        length = len(list(self.df['en']))\n",
    "        return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get row in df\n",
    "        dict_idx = dict(self.df.iloc[idx])\n",
    "\n",
    "        # Get labels as torch tensors\n",
    "        primary_label = torch.tensor([1 if dict_idx['en'] == label else 0 for label in self.species],dtype=float)\n",
    "\n",
    "        # Load audio\n",
    "        if dict_idx['en'] == 'nocall':\n",
    "            idx_dataset = 1\n",
    "        else:\n",
    "            idx_dataset = 0\n",
    "        ogg_file = os.path.join(self.audio_paths[idx_dataset],dict(self.df.iloc[idx])['filename'])\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        # Get clip of length self.period\n",
    "        target_audio_length = sample_rate * self.period\n",
    "        current_audio_length = len(waveform)\n",
    "        if current_audio_length > target_audio_length:\n",
    "            start = random.randint(0,current_audio_length - target_audio_length - 1)\n",
    "            waveform_seg = waveform[start:start+target_audio_length]\n",
    "        else:\n",
    "            padding_length = target_audio_length - current_audio_length\n",
    "            waveform_seg = torch.nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "        waveform_seg = resampler(waveform_seg)\n",
    "\n",
    "        if random.uniform(0,1) > self.config.addbackground_prob and self.backgroundData is not None:\n",
    "            idx = random.randint(0,len(self.backgroundData)-1)\n",
    "            backgroundNoise = self.backgroundData[idx][0]\n",
    "            waveform_seg += backgroundNoise * 0.2\n",
    "\n",
    "        # Apply transform\n",
    "        if self.audio_transforms is not None:\n",
    "            waveform_seg = self.audio_transforms(\n",
    "                samples=waveform_seg.detach().numpy(), sample_rate=self.config.sample_rate\n",
    "                )\n",
    "            \n",
    "        if isinstance(waveform_seg, np.ndarray):\n",
    "                waveform_seg = torch.from_numpy(waveform_seg)\n",
    "\n",
    "        return waveform_seg, primary_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Call Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoCallDataset(torch.nn.Module):\n",
    "    def __init__(self, datapath, metadata_df, audio_transforms, sample_rate, period, inherited_species_list=None, *args, **kwargs) -> None:\n",
    "\n",
    "        # Default values\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.period = period\n",
    "\n",
    "        self.df = metadata_df\n",
    "        self.datapath = datapath\n",
    "\n",
    "        self.soft_second_label = 0\n",
    "\n",
    "        self.audio_paths = [0, self.datapath[1]]\n",
    "\n",
    "\n",
    "        # Get species list\n",
    "        if inherited_species_list is None:\n",
    "            self.species = list(set(self.df['en']))\n",
    "        else:\n",
    "            self.species = inherited_species_list\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        # length = self.df['cumulative_images'][-1]\n",
    "        length = len(list(self.df['en']))\n",
    "        return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get row in df\n",
    "        dict_idx = dict(self.df.iloc[idx])\n",
    "\n",
    "        # Get labels as torch tensors\n",
    "        primary_label = torch.tensor([1 if dict_idx['en'] == label else 0 for label in self.species],dtype=float)\n",
    "        secondary_label = torch.tensor([1 if label in dict_idx['secondary_labels'] else 0 for label in self.species], dtype=float)\n",
    "        combined_label = self._prepare_target(main_tgt=primary_label, sec_tgt=secondary_label)\n",
    "        dict_idx['combined_label_tensor'] = combined_label\n",
    "        dict_idx['primary_label_tensor'] = primary_label\n",
    "        dict_idx['secondary_label_tensor'] = secondary_label\n",
    "\n",
    "        # Load audio\n",
    "        if dict_idx['en'] == 'nocall':\n",
    "            idx_dataset = 1\n",
    "        else:\n",
    "            idx_dataset = 0\n",
    "        ogg_file = os.path.join(self.audio_paths[idx_dataset],dict(self.df.iloc[idx])['filename'])\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        # Get clip of length self.period\n",
    "        target_audio_length = sample_rate * self.period\n",
    "        current_audio_length = len(waveform)\n",
    "        if current_audio_length > target_audio_length:\n",
    "            start = random.randint(0,current_audio_length - target_audio_length - 1)\n",
    "            waveform_seg = waveform[start:start+target_audio_length]\n",
    "        else:\n",
    "            padding_length = target_audio_length - current_audio_length\n",
    "            waveform_seg = torch.nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "        waveform_seg = resampler(waveform_seg)\n",
    "\n",
    "        return waveform_seg, combined_label\n",
    "\n",
    "    # https://github.com/VSydorskyy/BirdCLEF_2023_1st_place/blob/main/code_base/datasets/wave_dataset.py, changed\n",
    "    def _prepare_target(self, main_tgt, sec_tgt, all_labels=None):\n",
    "        all_tgt = main_tgt + sec_tgt * self.soft_second_label\n",
    "        all_tgt = torch.clamp(all_tgt, 0.0, 1.0)\n",
    "        return all_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train / Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/nischaydnk/split-creating-melspecs-stage-1\n",
    "def birds_stratified_split(df, target_col, test_size=0.2):\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    low_count_classes = class_counts[class_counts < 2].index.tolist() ### Birds with single counts\n",
    "\n",
    "    df['train'] = df[target_col].isin(low_count_classes)\n",
    "\n",
    "    train_df, val_df = train_test_split(df[~df['train']], test_size=test_size, stratify=df[~df['train']][target_col], random_state=42)\n",
    "\n",
    "    train_df = pd.concat([train_df, df[df['train']]], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Remove the 'valid' column\n",
    "    train_df.drop('train', axis=1, inplace=True)\n",
    "    val_df.drop('train', axis=1, inplace=True)\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uniformSampler(dataset, config, replacement = True):\n",
    "    classes_lsit = dataset.species\n",
    "\n",
    "    count_int = [0] * len(classes_lsit)\n",
    "    count_int_frame = [0] * len(classes_lsit)\n",
    "    nocall_count = 0\n",
    "\n",
    "    # Get class counts\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['en'] == 'nocall':\n",
    "            nocall_count += 1\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['en'])\n",
    "            count_int[species_index] += 1\n",
    "            count_int_frame[species_index] += dataset.df.iloc[i]['length']\n",
    "\n",
    "    # Calculate class weights\n",
    "    n_recs = sum(count_int)\n",
    "    n_frames = sum(count_int_frame)\n",
    "    n_nocall = nocall_count\n",
    "\n",
    "    # Consider if no nocall samples\n",
    "    if n_nocall == 0:\n",
    "        # n_nocall = 1\n",
    "        loc_frac_nocall = 0\n",
    "    else:\n",
    "        loc_frac_nocall = config.frac_nocall\n",
    "\n",
    "    sample_weights = [0] * len(dataset)\n",
    "\n",
    "    no_call_sum = 0\n",
    "    call_sum = 0\n",
    "\n",
    "    prob_per_class = 1 / len(dataset.df['en'].unique().tolist())\n",
    "\n",
    "\n",
    "    # Sample weights not considering recording length\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['en'] == 'nocall':\n",
    "            sample_weights[i] = 1 / n_nocall\n",
    "            no_call_sum += sample_weights[i]\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['en'])\n",
    "            sample_weights[i] = ( prob_per_class / count_int[species_index] )\n",
    "\n",
    "\n",
    "    # Modify weights to consider recording length\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['en'] == 'nocall':\n",
    "            pass\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['en'])\n",
    "            sample_weights[i] = sample_weights[i]  * ( dataset.df.iloc[i]['length'] / count_int_frame[species_index] ) * count_int[species_index]\n",
    "\n",
    "    # Normalize\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['en'] == 'nocall':\n",
    "            sample_weights[i] = sample_weights[i] * loc_frac_nocall\n",
    "            # sample_weights[i] = sample_weights[i] / no_call_sum * loc_frac_nocall\n",
    "        else:\n",
    "            sample_weights[i] = sample_weights[i] * (1 - loc_frac_nocall)\n",
    "            # sample_weights[i] = sample_weights[i] / call_sum * (1 - loc_frac_nocall)\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(dataset), replacement=replacement)\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_class_weights(dataset, focus = 'prevalence'):\n",
    "    if focus == 'prevalence':\n",
    "        n_data = len(dataset)\n",
    "        classes_lsit = dataset.species\n",
    "\n",
    "        count_int = [0] * len(classes_lsit)\n",
    "        nocall_count = 0\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset.df.iloc[i]['en'] == 'nocall':\n",
    "                nocall_count += 1\n",
    "            else:\n",
    "                species_index = classes_lsit.index(dataset.df.iloc[i]['en'])\n",
    "                count_int[species_index] += 1\n",
    "\n",
    "        n_call = sum(count_int)\n",
    "        class_weights = (np.array(count_int) / n_call) ** -0.5   * len(classes_lsit)\n",
    "\n",
    "    else:\n",
    "        class_weights = [0] * len(dataset)\n",
    "\n",
    "        class_weights = np.array(dataset.df['rating'].tolist())\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(config):\n",
    "    # Load dataframe\n",
    "    df = pd.read_csv(config.dataset_swiss_birds_training_metadata)\n",
    "\n",
    "    # Get Split\n",
    "    train_df, val_df = birds_stratified_split(df=df, target_col='en', test_size=config.val_frac)\n",
    "\n",
    "    # Get global species list\n",
    "    species_list = config.species_list\n",
    "\n",
    "    # Initialize Datasets\n",
    "    train_class_kwargs = {  'sample_rate': config.sample_rate,\n",
    "                            'n_fft': config.n_fft,\n",
    "                            'f_min': config.f_min,\n",
    "                            'f_max': config.f_max,\n",
    "                            'hop_length': config.hop_length,\n",
    "                            'n_mels': config.n_mels,\n",
    "                            'period': config.period,\n",
    "                            'device': config.device,\n",
    "                            'transform': config.train_transforms,\n",
    "                            'soft_second_label': config.soft_second_label\n",
    "                        }\n",
    "\n",
    "    valid_class_kwargs = {   'sample_rate': config.sample_rate,\n",
    "                            'n_fft': config.n_fft,\n",
    "                            'f_min': config.f_min,\n",
    "                            'f_max': config.f_max,\n",
    "                            'hop_length': config.hop_length,\n",
    "                            'n_mels': config.n_mels,\n",
    "                            'period': config.period,\n",
    "                            'device': config.device,\n",
    "                            'transform': config.val_transforms\n",
    "                        }\n",
    "\n",
    "    #Make No Call Dataframes\n",
    "    df_nocall = pd.read_csv(config.birdclef2021_background_noise_metadata)\n",
    "    # Rename columns \n",
    "    df_nocall = df_nocall.rename(columns={'primary_label': 'en', 'recordings_lengths': 'length'})\n",
    "    # Split\n",
    "    df_train_nocall, df_valid_nocall = train_test_split(df_nocall, test_size=config.val_frac)\n",
    "\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    df_train_full = pd.concat([train_df, df_train_nocall], axis = 0)\n",
    "    df_valid_full = pd.concat([val_df, df_valid_nocall], axis = 0)\n",
    "\n",
    "    # Reset the index to create a new index for the concatenated DataFrame\n",
    "    df_train_full.reset_index(drop=True, inplace=True)\n",
    "    df_valid_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Make Datasets\n",
    "    train_dataset_nocall = NoCallDataset(datapath=[0,config.birdclef2021_background_noise], metadata_df=df_train_nocall, audio_transforms=config.train_transforms_audio, sample_rate=config.sample_rate, period=config.period, inherited_species_list=species_list)\n",
    "    valid_dataset_nocall = NoCallDataset(datapath=[0,config.birdclef2021_background_noise], metadata_df=df_valid_nocall, audio_transforms=config.val_transforms_audio, sample_rate=config.sample_rate, period=config.period, inherited_species_list=species_list)\n",
    "\n",
    "    # Make dataset\n",
    "    if config.use_nocall == True:\n",
    "        train_dataset = SwissBirds(datapath=[config.dataset_swiss_birds_train, config.birdclef2021_background_noise], config=config, metadata_df=df_train_full, audio_transforms=config.train_transforms_audio ,sample_rate=config.sample_rate, soft_second_label=config.soft_second_label, period=config.period, inherited_species_list=species_list)\n",
    "        valid_dataset = SwissBirds(datapath=[config.dataset_swiss_birds_train, config.birdclef2021_background_noise], config=config, metadata_df=df_valid_full, audio_transforms=config.val_transforms_audio, sample_rate=config.sample_rate, soft_second_label=config.soft_second_label, period=config.period, inherited_species_list=species_list)\n",
    "    else:\n",
    "        train_dataset = SwissBirds(datapath=[config.dataset_swiss_birds_train], metadata_df=train_df, config=config, audio_transforms=config.train_transforms_audio ,sample_rate=config.sample_rate, soft_second_label=config.soft_second_label, period=config.period, inherited_species_list=species_list, backgroundData=train_dataset_nocall)\n",
    "        valid_dataset = SwissBirds(datapath=[config.dataset_swiss_birds_train], metadata_df=val_df, config=config, audio_transforms=config.val_transforms_audio, sample_rate=config.sample_rate, soft_second_label=config.soft_second_label, period=config.period, inherited_species_list=species_list, backgroundData=valid_dataset_nocall)\n",
    "\n",
    "\n",
    "    if config.uniform_sampler == False:\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,num_workers=config.num_workers, batch_size=config.train_batch_size, shuffle = True, pin_memory = True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,num_workers=config.num_workers, batch_size=config.valid_batch_size, shuffle = True, pin_memory = True)\n",
    "    else:\n",
    "        train_sampler = make_uniformSampler(dataset=train_dataset, config=config, replacement = config.with_replacement_train)\n",
    "        valid_sampler = make_uniformSampler(dataset=valid_dataset, config=config, replacement = config.with_replacement_valid)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,num_workers=config.num_workers, batch_size=config.train_batch_size, pin_memory = True, sampler=train_sampler)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,num_workers=config.num_workers, batch_size=config.valid_batch_size, pin_memory = True, sampler=valid_sampler)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric Function as on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_cmap(solution, submission, padding_factor=5):\n",
    "    solution = solution.drop(['row_id'], axis=1, errors='ignore')\n",
    "    submission = submission.drop(['row_id'], axis=1, errors='ignore')\n",
    "    new_rows = []\n",
    "    for i in range(padding_factor):\n",
    "        new_rows.append([1 for i in range(len(solution.columns))])\n",
    "    new_rows = pd.DataFrame(new_rows)\n",
    "    new_rows.columns = solution.columns\n",
    "    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n",
    "    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n",
    "    score = sklearn.metrics.average_precision_score(\n",
    "        padded_solution.values,\n",
    "        padded_submission.values,\n",
    "        average='macro',\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, smooth_eps=0.0025, weight=None, reduction=\"mean\"):\n",
    "        super(LabelSmoothingBCEWithLogitsLoss, self).__init__()\n",
    "        self.smooth_eps = smooth_eps\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.bce_with_logits_loss = nn.BCEWithLogitsLoss(weight=self.weight, reduction=self.reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target_smooth = torch.clamp(target.float(), self.smooth_eps, 1.0 - self.smooth_eps)\n",
    "        target_smooth = target_smooth + (self.smooth_eps / target.size(1))\n",
    "        return self.bce_with_logits_loss(input, target_smooth)\n",
    "    \n",
    "class BCEFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0,weight = None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(weight=self.weight, reduction='none')(preds, targets)\n",
    "        probas = torch.sigmoid(preds)\n",
    "        loss = targets * self.alpha * \\\n",
    "            (1. - probas)**self.gamma * bce_loss + \\\n",
    "            (1. - targets) * probas**self.gamma * bce_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.25,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"mean\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "            inputs=inputs,\n",
    "            targets=targets,\n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(nn.Module):\n",
    "    def __init__(self, mix_beta):\n",
    "\n",
    "        super(Mixup, self).__init__()\n",
    "        self.beta_distribution = torch.distributions.Beta(mix_beta, mix_beta)\n",
    "\n",
    "    def forward(self, X, Y, weight=None, teacher_preds=None):\n",
    "\n",
    "        bs = X.shape[0]\n",
    "        n_dims = len(X.shape)\n",
    "        perm = torch.randperm(bs)\n",
    "        coeffs = self.beta_distribution.rsample(torch.Size((bs,))).to(X.device)\n",
    "\n",
    "        if n_dims == 2:\n",
    "            X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n",
    "        elif n_dims == 3:\n",
    "            X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n",
    "        else:\n",
    "            X = coeffs.view(-1, 1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n",
    "\n",
    "        Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n",
    "\n",
    "        if weight is None:\n",
    "            return X, Y\n",
    "        else:\n",
    "            weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n",
    "            teacher_preds = coeffs.view(-1, 1) * teacher_preds + (1 - coeffs.view(-1, 1)) * teacher_preds[perm]\n",
    "            return X, Y, weight, teacher_preds\n",
    "        \n",
    "def train_net(net, trainloader, valloader, criterion, optimizer, scheduler, config, epochs=1, patience = 3, savePth = 'project2_weights.pth', print_every_samples = 20, device = 'cpu'):\n",
    "\n",
    "    mixup = Mixup(mix_beta=1)\n",
    "\n",
    "    print('Using device: {}'.format(device))\n",
    "    net.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    if config.softmax_prob == True:\n",
    "        toProb = torch.nn.Softmax(dim=1)\n",
    "    else:\n",
    "        toProb = torch.nn.Identity()\n",
    "\n",
    "    validation_loss_list = [0] * epochs\n",
    "    training_loss_list = [0] * epochs\n",
    "    validation_accuracy_list = [0] * epochs\n",
    "    training_accuracy_list = [0] * epochs\n",
    "    cmap_5_list = [0] * epochs\n",
    "\n",
    "    best_state_dictionary = None\n",
    "    best_validation_cmap = 0.0\n",
    "    inertia = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        training_accuracy = 0.0\n",
    "        running_loss = 0.0\n",
    "        # Set model to training mode\n",
    "        net.mel_generator.transform = config.train_transforms\n",
    "        net = net.train()\n",
    "\n",
    "        # Calculate the number of batches to loop over\n",
    "        num_batches_to_loop = int(config.training_data_per_epoch * len(trainloader))\n",
    "        with tqdm(enumerate(trainloader, 0), total=num_batches_to_loop, desc=\"Training Batches Epoch {} / {}\".format(epoch + 1, epochs)) as train_pbar:\n",
    "            for i, data in train_pbar:\n",
    "        \n",
    "                # get the inputs\n",
    "                if device == 'cuda':\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                else:\n",
    "                    inputs, labels = data[0], data[1]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # mixup data\n",
    "                if np.random.random() <= config.mix_up_prob:\n",
    "                    inputs, labels = mixup(inputs, labels)\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss_value = criterion((outputs),labels)\n",
    "\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics and write to log\n",
    "                running_loss += loss_value.item()\n",
    "                training_loss += loss_value.item()\n",
    "\n",
    "                train_pbar.set_postfix(loss=(running_loss / ((i + 1) * trainloader.batch_size)))\n",
    "                training_accuracy += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "\n",
    "                if type(scheduler).__name__ != 'NoneType':\n",
    "                    scheduler.step(epoch + i / len(trainloader))\n",
    "\n",
    "                if i >= num_batches_to_loop:\n",
    "                    break\n",
    "\n",
    "        training_loss = training_loss / (len(trainloader.dataset) * config.training_data_per_epoch)\n",
    "        training_loss_list[epoch] = training_loss\n",
    "        training_accuracy = 100 * training_accuracy / (len(trainloader.dataset) * config.training_data_per_epoch)\n",
    "        training_accuracy_list[epoch] = training_accuracy\n",
    "\n",
    "        print('Batch {:5d} / {:5d}: Training Loss = {:.3f}, Training Accuracy = {:.3f}'.format(epoch + 1, epochs, training_loss, training_accuracy))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        predictions_array = np.zeros((len(valloader.dataset), len(valloader.dataset.species)), dtype=float)\n",
    "        solutions_array = np.zeros((len(valloader.dataset), len(valloader.dataset.species)), dtype=float)\n",
    "        # Set model to validation mode\n",
    "        net.mel_generator.transform = config.val_transforms\n",
    "        net = net.eval()\n",
    "        with tqdm(enumerate(valloader, 0), total=len(valloader), desc=\"Validation Batches Epoch {} / {}\".format(epoch + 1, epochs)) as val_pbar:\n",
    "            for i, data in val_pbar:\n",
    "                # get the inputs\n",
    "                if device == 'cuda':\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                else:\n",
    "                    inputs, labels = data[0], data[1]\n",
    "                    \n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss_value = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics and write to log\n",
    "                running_loss += loss_value.item()\n",
    "                val_loss += loss_value.item()\n",
    "\n",
    "                # Get model output and label to array\n",
    "                curr_predictions_array = toProb(outputs).detach().cpu().numpy()\n",
    "                predictions_array[i*valloader.batch_size:(i+1)*valloader.batch_size,:] = curr_predictions_array\n",
    "                hardlabels = labels.detach().cpu().numpy()\n",
    "                hardlabels[hardlabels < 0.99] = 0\n",
    "                curr_solutions_array = hardlabels\n",
    "                solutions_array[i*valloader.batch_size:(i+1)*valloader.batch_size,:] = curr_solutions_array\n",
    "\n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix(loss=(running_loss / ((i + 1) * trainloader.batch_size)))\n",
    "                correct += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "        \n",
    "        # Get cMAP\n",
    "        cmap_5 = padded_cmap(solution=pd.DataFrame(solutions_array), submission=pd.DataFrame(predictions_array), padding_factor=5)\n",
    "\n",
    "        # Get Metrics\n",
    "        val_loss = val_loss / len(valloader.dataset)\n",
    "        validation_loss_list[epoch] = val_loss\n",
    "        val_accuracy = 100 * correct / len(valloader.dataset)\n",
    "        validation_accuracy_list[epoch] = val_accuracy\n",
    "        cmap_5_list[epoch] = cmap_5\n",
    "\n",
    "        print('Batch {:5d} / {:5d}: Validation Loss = {:.3f}, Validation Accuracy = {:.3f}, cmap score = {:.3f}'.format(epoch + 1, epochs, val_loss, val_accuracy, cmap_5))\n",
    "\n",
    "        save_weights = os.path.join(savePth,'model_weights.pth')\n",
    "        if cmap_5 > best_validation_cmap:\n",
    "            best_validation_cmap = cmap_5\n",
    "            best_state_dictionary = copy.deepcopy(net.state_dict())\n",
    "            # save network\n",
    "            torch.save(best_state_dictionary, save_weights)\n",
    "            inertia = 0\n",
    "            print('Epoch {:5d} / {:5d} saved: New Best Epoch!'.format(epoch + 1, epochs))\n",
    "        else:\n",
    "            inertia += 1\n",
    "            if inertia == patience:\n",
    "                if best_state_dictionary is None:\n",
    "                    raise Exception(\"State dictionary should have been updated at least once\")\n",
    "                break\n",
    "        # print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    output = {'validation_loss': validation_loss_list,\n",
    "              'validation_accuracy': validation_accuracy_list,\n",
    "              'training_loss': training_loss_list,\n",
    "              'training_accuracy': training_accuracy_list,\n",
    "              'cmap_5_scores': cmap_5_list}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    melspec_layer = MelSpectrogramLayer(sample_rate=config.sample_rate,\n",
    "                                    n_fft=config.n_fft,\n",
    "                                    hop_length=config.hop_length,\n",
    "                                    config=config,\n",
    "                                    n_mels=config.n_mels,\n",
    "                                    transform=config.train_transforms)\n",
    "    network = Mel_Classifier(model_name=config.model_name,\n",
    "                            mel_generator=melspec_layer)\n",
    "    \n",
    "    if config.load_pretrained_weights == True:\n",
    "        print('Load PreTrained Weigths')\n",
    "        network.load_state_dict(torch.load(config.pretrained_weights, map_location=config.device))\n",
    "\n",
    "    if config.fix_features == True:\n",
    "        for param in network.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return network\n",
    "\n",
    "def get_criterion(config, train_dataset = None):\n",
    "    \n",
    "    if config.criterion == 'LabelSmoothingBCEWithLogitsLoss':\n",
    "        if config.class_weighting == True:\n",
    "            class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "            criterion = LabelSmoothingBCEWithLogitsLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = LabelSmoothingBCEWithLogitsLoss()\n",
    "\n",
    "    if config.criterion == 'CrossEntropyLoss':\n",
    "        if config.class_weighting == True:\n",
    "            class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if config.criterion == 'BCEWithLogitsLoss':\n",
    "        if config.class_weighting == True:\n",
    "            class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "            criterion = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if config.criterion == 'BCEFocalLoss':\n",
    "        if config.class_weighting == True:\n",
    "            class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "            criterion = BCEFocalLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = BCEFocalLoss()\n",
    "\n",
    "    if config.criterion == 'FocalLoss':\n",
    "        if config.class_weighting == True:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            criterion = FocalLoss()\n",
    "\n",
    "    return criterion\n",
    "\n",
    "def get_optimizer(config, params):\n",
    "\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, params), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, params), lr=config.lr, momentum=config.momentum)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(config, optimizer):\n",
    "\n",
    "    if config.scheduler == 'cosineannealing':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                            optimizer, \n",
    "                            T_0=config.epochs, \n",
    "                            T_mult=config.T_mult, \n",
    "                            eta_min=config.eta_min, \n",
    "                            last_epoch=config.last_epoch\n",
    "                        )\n",
    "    elif config.scheduler == None:\n",
    "        scheduler = None\n",
    "\n",
    "    return scheduler\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(config, timefolder = True):\n",
    "\n",
    "    if timefolder == True:\n",
    "        # Change Output path\n",
    "        folder_name = time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        outpath = os.path.join(config.outpath, folder_name)\n",
    "        config.outpath = outpath\n",
    "\n",
    "    # Create Output directory\n",
    "    os.makedirs(config.outpath, exist_ok=True)\n",
    "\n",
    "    # Get all variables to json file\n",
    "    config_dict = {attr: value for attr, value in vars(config).items()}\n",
    "    outputName = 'hyperparameters.json'\n",
    "    jsonpath = os.path.join(config.outpath, outputName)\n",
    "    with open(jsonpath, 'w') as json_file:\n",
    "        json.dump(str(config_dict), json_file)\n",
    "\n",
    "    # Get model\n",
    "    network = get_model(config = config)\n",
    "    \n",
    "    # Get loss function\n",
    "    criterion = get_criterion(config = config)\n",
    "    \n",
    "    # Get optimizer\n",
    "    optimizer = get_optimizer(config = config, params = network.parameters())\n",
    "\n",
    "    # Get scheduler\n",
    "    scheduler = get_scheduler(config = config, optimizer = optimizer)\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_loader, valid_loader = make_dataset(config)\n",
    "\n",
    "    # Train Net\n",
    "    output = train_net( net=network,\n",
    "                        trainloader=train_loader,\n",
    "                        valloader=valid_loader,\n",
    "                        criterion=criterion,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        epochs=config.epochs,\n",
    "                        config=config,\n",
    "                        device=config.device,\n",
    "                        print_every_samples=config.print_every_n_batches,\n",
    "                        savePth=config.outpath,\n",
    "                        patience=config.patience\n",
    "                        )\n",
    "    \n",
    "    # Save Output\n",
    "    outputName = 'training_prog.json'\n",
    "    jsonpath = os.path.join(config.outpath, outputName)\n",
    "    with open(jsonpath, 'w') as json_file:\n",
    "        json.dump(output, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwissBirds_TestSet(torch.nn.Module):\n",
    "    def __init__(self, datapath, metadata_df, audio_transforms, sample_rate, period, inherited_species_list, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Default values\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.period = period\n",
    "\n",
    "\n",
    "        self.df = metadata_df\n",
    "        self.datapath = datapath\n",
    "       \n",
    "        self.species = inherited_species_list\n",
    "\n",
    "        self.make_cumulative_list()\n",
    "\n",
    "        return\n",
    "    \n",
    "    def make_cumulative_list(self):\n",
    "\n",
    "        length_list_seconds = np.array(list(self.df['length']))\n",
    "        num_clips_per_recording = (length_list_seconds // self.period).astype(int)\n",
    "        num_clips_per_recording = num_clips_per_recording.tolist()\n",
    "        for i in range(len(num_clips_per_recording)):\n",
    "            if num_clips_per_recording[i] == 0:\n",
    "                num_clips_per_recording[i] = 1\n",
    "        cum_list = [0] * len(num_clips_per_recording)\n",
    "        cumulative_sum = 0\n",
    "\n",
    "        for i,num in enumerate(num_clips_per_recording):\n",
    "            cumulative_sum += num\n",
    "            cum_list[i] = cumulative_sum\n",
    "\n",
    "        self.df['cum_num_clips'] = cum_list\n",
    "        self.df['num_clips'] = num_clips_per_recording\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        length = self.df['cum_num_clips'].iloc[-1]\n",
    "        return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx_glob):\n",
    "\n",
    "        # Get pd idx\n",
    "        idx = bisect.bisect_left(self.df['cum_num_clips'], idx_glob + 1)\n",
    "\n",
    "        # Get local idx\n",
    "        if idx != 0:\n",
    "            loc_idx = idx_glob - self.df['cum_num_clips'].iloc[idx - 1]\n",
    "        else:\n",
    "            loc_idx = idx_glob\n",
    "\n",
    "        # Get row in df\n",
    "        dict_idx = dict(self.df.iloc[idx])\n",
    "\n",
    "        # Get labels as torch tensors\n",
    "        primary_label = torch.tensor([1 if dict_idx['en'] == label else 0 for label in self.species],dtype=float)\n",
    "\n",
    "        # Load audio\n",
    "        ogg_file = os.path.join(self.datapath,dict(self.df.iloc[idx])['filename'])\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        # Get clip of length self.period\n",
    "        target_audio_length = sample_rate * self.period\n",
    "        current_audio_length = len(waveform)\n",
    "        if current_audio_length >= target_audio_length:\n",
    "            start = loc_idx * target_audio_length\n",
    "            waveform_seg = waveform[start:start+target_audio_length]\n",
    "        else:\n",
    "            padding_length = target_audio_length - current_audio_length\n",
    "            waveform_seg = torch.nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "        waveform_seg = resampler(waveform_seg)\n",
    "\n",
    "        # Apply transform\n",
    "        if self.audio_transforms is not None:\n",
    "            waveform_seg = self.audio_transforms(waveform_seg.unsqueeze(0)).ravel()\n",
    "\n",
    "        return waveform_seg, primary_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testSet(config):\n",
    "    test_df = pd.read_csv(config.dataset_swiss_birds_test_metadata)\n",
    "    test_dataset = SwissBirds_TestSet(datapath=config.dataset_swiss_birds_test, metadata_df=test_df, audio_transforms=config.test_transforms_audio, sample_rate=config.sample_rate, period=config.period, inherited_species_list=config.species_list)\n",
    "    test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,num_workers=config.num_workers, batch_size=config.test_batch_size, shuffle = False, pin_memory = True)\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_model(config):\n",
    "    melspec_layer = MelSpectrogramLayer(sample_rate=config.sample_rate,\n",
    "                                    n_fft=config.n_fft,\n",
    "                                    hop_length=config.hop_length,\n",
    "                                    n_mels=config.n_mels,\n",
    "                                    config=config,\n",
    "                                    transform=config.test_transforms)\n",
    "    network = Mel_Classifier(model_name=config.model_name,\n",
    "                            mel_generator=melspec_layer)\n",
    "    \n",
    "    print('Load PreTrained Weigths')\n",
    "    weights_loc = os.path.join(config.outpath,'model_weights.pth')\n",
    "    network.load_state_dict(torch.load(weights_loc, map_location=config.device))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "def plot_data(file_path: Path, savePath):\n",
    "\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    validation_accuracy, validation_loss, training_accuracy, training_loss = data['validation_accuracy'], data['validation_loss'], data['training_accuracy'], data['training_loss']\n",
    "    epochs = range(1, len(validation_accuracy) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(21, 7))\n",
    "\n",
    "    ax1.plot(epochs, training_loss, label=\"Training\")\n",
    "    ax1.plot(epochs, validation_loss, label=\"Validation\")\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss [-]\")\n",
    "    ax1.grid(\"on\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(epochs, training_accuracy, label=\"Training\")\n",
    "    ax2.plot(epochs, validation_accuracy, label=\"Validation\")\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy [%]\")\n",
    "    ax2.grid(\"on\")\n",
    "    ax2.legend()\n",
    "\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plt.savefig(savePath, dpi=600,  bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "# https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7\n",
    "def getConfusionMatrix(y_pred, y_true, name):\n",
    "    \n",
    "    # constant for classes\n",
    "    classes = CONFIG.species_list\n",
    "\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
    "                        columns = [i for i in classes])\n",
    "    \n",
    "    plt.figure(figsize = (12,8))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(name, bbox_inches = 'tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(config):\n",
    "\n",
    "    network = get_test_model(config).to(config.device)\n",
    "    test_dataloader = get_testSet(config)\n",
    "\n",
    "    toProb = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    correct = 0\n",
    "    predictions_array = np.zeros((len(test_dataloader.dataset), len(test_dataloader.dataset.species)), dtype=float)\n",
    "    solutions_array = np.zeros((len(test_dataloader.dataset), len(test_dataloader.dataset.species)), dtype=float)\n",
    "    network.eval()\n",
    "    # for i,data in enumerate(test_dataloader):\n",
    "    with tqdm(enumerate(test_dataloader, 0), total=len(test_dataloader), desc=\"Test Set\") as val_pbar:\n",
    "            for i, data in val_pbar:\n",
    "\n",
    "                if config.device == 'cuda':\n",
    "                    inputs, labels = data[0].to(config.device), data[1].to(config.device)\n",
    "                else:\n",
    "                    inputs, labels = data[0], data[1]\n",
    "                        \n",
    "                outputs = network(inputs)\n",
    "\n",
    "                # Get model output and label to array\n",
    "                curr_predictions_array = toProb(outputs).detach().cpu().numpy()\n",
    "                predictions_array[i*test_dataloader.batch_size:(i+1)*test_dataloader.batch_size,:] = curr_predictions_array\n",
    "                hardlabels = labels.detach().cpu().numpy()\n",
    "                hardlabels[hardlabels < 0.99] = 0\n",
    "                curr_solutions_array = hardlabels\n",
    "                solutions_array[i*test_dataloader.batch_size:(i+1)*test_dataloader.batch_size,:] = curr_solutions_array\n",
    "\n",
    "                # Update progress bar\n",
    "                correct += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "\n",
    "    # Get cMAP\n",
    "    cmap_5 = padded_cmap(solution=pd.DataFrame(solutions_array), submission=pd.DataFrame(predictions_array), padding_factor=5)\n",
    "\n",
    "    # Get Accuracy\n",
    "    test_accuracy = 100 * correct / len(test_dataloader.dataset)\n",
    "\n",
    "    print('cmap_5 value is {}'.format(cmap_5))\n",
    "    print('Test Accuracy is {}'.format(test_accuracy))\n",
    "\n",
    "    # Save results\n",
    "    result_dict = {'test_accuracy': test_accuracy, \n",
    "                'cmap_5': cmap_5}\n",
    "    outputName = 'results.json'\n",
    "    jsonpath = os.path.join(config.outpath, outputName)\n",
    "    with open(jsonpath, 'w') as json_file:\n",
    "        json.dump(str(result_dict), json_file)\n",
    "\n",
    "\n",
    "    y_pred = predictions_array.argmax(1)\n",
    "    y_true = solutions_array.argmax(1)\n",
    "\n",
    "    confmatname = os.path.join(config.outpath,'confusion_matrix.png')\n",
    "    plot_name = os.path.join(config.outpath, 'trainingPlot.png')\n",
    "    jsonpath = os.path.join(config.outpath, 'training_prog.json')\n",
    "    getConfusionMatrix(y_pred=y_pred,y_true=y_true, name=confmatname)\n",
    "    plot_data(file_path=jsonpath, savePath=plot_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_replacement = [True, False]\n",
    "loss_function = ['CrossEntropyLoss', 'LabelSmoothingBCEWithLogitsLoss'] # 'BCEFocalLoss' # 'LabelSmoothingBCEWithLogitsLoss' # 'CrossEntropyLoss' # 'BCEWithLogitsLoss'\n",
    "augment_audio_train = [True, False]\n",
    "training_audio_period = [5]\n",
    "\n",
    "ittt = 0\n",
    "for replacement_bool in with_replacement:\n",
    "    for loss_fun in loss_function:\n",
    "        for augment_aud_bool in augment_audio_train:\n",
    "            for period in training_audio_period:\n",
    "\n",
    "                ittt += 1\n",
    "\n",
    "                print('#########################################################################')\n",
    "                print('#########################  Running Training {}  #########################'.format(ittt))\n",
    "                print('#########################################################################')\n",
    "\n",
    "                CONFIG_GRID = Config()\n",
    "\n",
    "                exp_name = 'with_replacement_' + str(replacement_bool) + '__loss_function_' + str(loss_fun) + '__augment_audio_train_' + str(augment_aud_bool) + '__training_audio_period_' + str(training_audio_period)\n",
    "\n",
    "                CONFIG_GRID.with_replacement_train = replacement_bool\n",
    "                CONFIG_GRID.with_replacement_valid = replacement_bool\n",
    "                CONFIG_GRID.uniform_sampler = replacement_bool\n",
    "\n",
    "                CONFIG_GRID.criterion = loss_fun\n",
    "\n",
    "                CONFIG_GRID.augment_audio_train = augment_aud_bool\n",
    "\n",
    "                CONFIG_GRID.outpath = os.path.join(CONFIG_GRID.outpath,exp_name)\n",
    "\n",
    "                CONFIG_GRID.period = period\n",
    "\n",
    "                factor = period // CONFIG.period\n",
    "\n",
    "                CONFIG_GRID.train_transforms = torchvision.transforms.Compose([\n",
    "                            torchaudio.transforms.AmplitudeToDB(),\n",
    "                            # torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                            torchvision.transforms.RandomResizedCrop(size=(80*factor,78*factor), scale = (0.75, 1.0), antialias=True), \n",
    "                            ])\n",
    "                \n",
    "                CONFIG_GRID.val_transforms = torchvision.transforms.Compose([\n",
    "                            torchaudio.transforms.AmplitudeToDB(),\n",
    "                            # torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                            torchvision.transforms.Resize(size=[80*factor,78*factor], antialias=True),\n",
    "                            # torchvision.transforms.RandomResizedCrop(size=(128, 312), scale = (0.75, 1.0), antialias=True), \n",
    "                            ])\n",
    "\n",
    "                #Train\n",
    "                main_train(config=CONFIG_GRID, timefolder=False)\n",
    "\n",
    "\n",
    "                print('#########################################################################')\n",
    "                print('#########################  Running Test {}  #########################'.format(ittt))\n",
    "                print('#########################################################################')\n",
    "\n",
    "                #Test\n",
    "                CONFIG_GRID.period = CONFIG.period\n",
    "                run_test(config=CONFIG_GRID)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle: Zip Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# https://www.kaggle.com/code/hari31416/downloading-file-and-directory-from-kaggle\n",
    "def zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n",
    "    \"\"\"\n",
    "    zip all the files in a directory\n",
    "    \n",
    "    Parameters\n",
    "    _____\n",
    "    directory: str\n",
    "        directory needs to be zipped, defualt is current working directory\n",
    "        \n",
    "    file_name: str\n",
    "        the name of the zipped file (including .zip), default is 'directory.zip'\n",
    "        \n",
    "    Returns\n",
    "    _____\n",
    "    Creates a hyperlink, which can be used to download the zip file)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)\n",
    "    zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file_name in file:\n",
    "                pass\n",
    "            else:\n",
    "                zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "    return FileLink(file_name)\n",
    "\n",
    "if CONFIG.run_environment == 'kaggle':\n",
    "    zip_dir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
