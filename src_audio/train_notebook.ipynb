{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook for BirdCLEF2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all Dependencies (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/elec5305/ele5305_research_project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import bisect\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CONFIG class containing all relevant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, smooth_eps=0.0025, weight=None, reduction=\"mean\"):\n",
    "        super(LabelSmoothingBCEWithLogitsLoss, self).__init__()\n",
    "        self.smooth_eps = smooth_eps\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.bce_with_logits_loss = nn.BCEWithLogitsLoss(weight=self.weight, reduction=self.reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target_smooth = torch.clamp(target.float(), self.smooth_eps, 1.0 - self.smooth_eps)\n",
    "        target_smooth = target_smooth + (self.smooth_eps / target.size(1))\n",
    "        return self.bce_with_logits_loss(input, target_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE UNLESS RE-GENERATING DATASET IMAGES ###\n",
    "class Config_Mel():\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        # Device\n",
    "        self.device = 'cpu'\n",
    "        \n",
    "        # Dataset Path\n",
    "        self.birdclef2023 = 'birdclef-2023'\n",
    "\n",
    "        # Out path\n",
    "        self.outpath_images = self.birdclef2023 + '_MelSpectrograms'\n",
    "\n",
    "        self.melSpecTransform = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        # Audio Features\n",
    "        self.sample_rate = 32000\n",
    "        self.n_fft=2048\n",
    "        self.f_min=40\n",
    "        self.f_max=15000\n",
    "        self.hop_length=512\n",
    "        self.n_mels=128\n",
    "        self.mel_args = {'sample_rate': self.sample_rate,\n",
    "                         'n_fft': self.n_fft,\n",
    "                         'f_min': self.f_min,\n",
    "                         'f_max': self.f_max,\n",
    "                         'hop_length': self.hop_length,\n",
    "                         'n_mels': self.n_mels}\n",
    "### DO NOT CHANGE UNLESS RE-GENERATING DATASET IMAGES ###\n",
    "\n",
    "class Config():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.run_environment = 'local' # 'kaggle' 'local' 'colab'\n",
    "        self.load_pretrained_weights = False\n",
    "        self.rerun_split = False\n",
    "        self.use_5_second_dataset = True\n",
    "        self.uniform_sampler = True\n",
    "        self.training_data_per_epoch = 0.25\n",
    "        self.soft_second_label = 0.3\n",
    "        self.class_weighting = True\n",
    "        self.softmax_prob = True\n",
    "        self.use_mixup = True\n",
    "        self.criterion = 'LabelSmoothingBCEWithLogitsLoss' # 'LabelSmoothingBCEWithLogitsLoss' # 'CrossEntropyLoss' # 'BCEWithLogitsLoss'\n",
    "        self.epochs = 25\n",
    "        self.masking = True\n",
    "        self.masking_prob = 0.5\n",
    "        self.frac_nocall = 0.01\n",
    "        self.use_nocall = False\n",
    "        self.addbackground_prob = 0.5\n",
    "\n",
    "        # Device\n",
    "        if (self.run_environment == 'kaggle') or (self.run_environment == 'colab'):\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        # Dataset Path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.birdclef2023_melspectrograms = '/kaggle/input/birdclef-2023-melspectrograms/birdclef-2023_MelSpectrograms'\n",
    "            self.birdclef2023_melspectrograms_5_seconds = '/kaggle/input/birdclef-2023-melspectrograms-5-seconds/birdclef-2023_MelSpectrograms_5_seconds'\n",
    "            self.birdclef2023 = '/kaggle/input/birdclef-2023'\n",
    "            self.birdclef2021_background_noise = '/kaggle/input/birdclef2021-background-noise/ff1010bird_nocall'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.birdclef2023_melspectrograms = '/home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms'\n",
    "            self.birdclef2023_melspectrograms_5_seconds = '/home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms_5_seconds'\n",
    "            self.birdclef2023 = '/home/colin/elec5305/ele5305_research_project/birdclef-2023'\n",
    "            self.birdclef2021_background_noise = '/home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall'\n",
    "        elif self.run_environment == 'colab':\n",
    "            self.birdclef2023_melspectrograms = ''\n",
    "\n",
    "        # Out path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.outpath = '/kaggle/working/results'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.outpath = 'results'\n",
    "        elif self.run_environment == 'colab':\n",
    "            self.outpath = ''\n",
    "\n",
    "        # Train/Validation Split \n",
    "        self.val_frac = 0.1\n",
    "\n",
    "        # Dataloader options\n",
    "        self.num_workers = 2\n",
    "        self.train_batch_size = 64\n",
    "        self.valid_batch_size = 32\n",
    "\n",
    "        # Model name\n",
    "        self.model_name = 'tf_efficientnet_b0.ns_jft_in1k'\n",
    "        # Pretrained\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.pretrained_weights = '/kaggle/input/weights12/model_weights.pth'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.pretrained_weights = '/home/colin/elec5305/ele5305_research_project/weights/model_weights.pth'\n",
    "\n",
    "\n",
    "        # Optimizer Settings\n",
    "        self.lr=5e-4\n",
    "        self.weight_decay = 1e-3\n",
    "        self.momentum=0.9\n",
    "        self.optimizer = 'adam' # 'adam', 'sgd'\n",
    "\n",
    "        self.scheduler = 'cosineannealing'\n",
    "        self.eta_min = 1e-6\n",
    "        self.T_mult = 1\n",
    "        self.last_epoch = -1\n",
    "\n",
    "        self.mixup_alpha = 0.5\n",
    "\n",
    "        # Training Settings\n",
    "        self.print_every_n_batches = 25\n",
    "        self.patience = 5\n",
    "        self.fix_features = False\n",
    "\n",
    "        # Image Transforms\n",
    "        self.train_transforms = torchvision.transforms.Compose([\n",
    "                    torchaudio.transforms.AmplitudeToDB(),\n",
    "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.RandomResizedCrop(size=(128, 312), scale = (0.75, 1.0), antialias=True), \n",
    "                    ])\n",
    "        \n",
    "        self.val_transforms = torchvision.transforms.Compose([\n",
    "                    torchaudio.transforms.AmplitudeToDB(),\n",
    "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.Resize(size=[128,312], antialias=True),\n",
    "                    # torchvision.transforms.RandomResizedCrop(size=(128, 312), scale = (0.75, 1.0), antialias=True), \n",
    "                    ])\n",
    "        \n",
    "        self.test_transforms = torchvision.transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    # torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    # torchvision.transforms.Resize(size=(224, 224), antialias=True),  # Or Resize(antialias=True)\n",
    "                    ])\n",
    "\n",
    "        # Audio Transforms\n",
    "        self.train_transforms_audio = None\n",
    "        \n",
    "        self.val_transforms_audio = None\n",
    "        \n",
    "        self.test_transforms_audio = None\n",
    "\n",
    "\n",
    "        # Audio Features\n",
    "        self.sample_rate = 32000\n",
    "        self.period = 5\n",
    "\n",
    "        # Mel Spectrogram Parameters\n",
    "        self.n_fft=2048\n",
    "        self.f_min=40\n",
    "        self.f_max=15000\n",
    "        self.hop_length=512\n",
    "        self.n_mels=128\n",
    "        self.mel_args = {'n_fft': self.n_fft,\n",
    "                         'f_min': self.f_min,\n",
    "                         'f_max': self.f_max,\n",
    "                         'hop_length': self.hop_length,\n",
    "                         'n_mels': self.n_mels}\n",
    "        \n",
    "        if self.run_environment == 'colab':\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "\n",
    "            notebook_path = 'My Drive/elec5305'\n",
    "\n",
    "            env_path = f'/content/drive/{notebook_path}'\n",
    "            # Add the handout folder to python paths\n",
    "            if env_path not in sys.path:\n",
    "                sys.path.append(env_path)\n",
    "\n",
    "            # zip_path = os.path.join(env_path, 'birdclef-2023_MelSpectrograms.zip')\n",
    "            zip_path = '/content/drive/MyDrive/elec5305/birdclef-2023_MelSpectrograms.zip'\n",
    "            shutil.unpack_archive(zip_path, \"content/\")\n",
    "            print(zip_path)\n",
    "            # !unzip zip_path -d \"/content\"\n",
    "\n",
    "            # Dataset path\n",
    "            self.birdclef2023_melspectrograms = '/content/content/birdclef-2023_MelSpectrograms'\n",
    "\n",
    "            # Output path\n",
    "            self.outpath = os.path.join(env_path, 'results')\n",
    "            os.makedirs(self.outpath, exist_ok=True)# !pip install --force-reinstall numpy==1.22.1\n",
    "            \n",
    "            %pip install -q torchtoolbox timm\n",
    "\n",
    "            %pip install timm torchtoolbox\n",
    "\n",
    "        if self.run_environment != 'local':\n",
    "            # !pip install --force-reinstall numpy==1.22.1\n",
    "            %pip install -q torchtoolbox timm\n",
    "        \n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all dependencies (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtoolbox.tools import mixup_data, mixup_criterion\n",
    "from torch.nn.functional import cross_entropy\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramLayer(nn.Module):\n",
    "    def __init__(self, sample_rate, n_fft, hop_length, n_mels, transform):\n",
    "        super(MelSpectrogramLayer, self).__init__()\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        mel_spectrogram = self.mel_transform(waveform)\n",
    "\n",
    "        batched = True\n",
    "        if mel_spectrogram.dim() == 2:\n",
    "            batched = False\n",
    "        elif mel_spectrogram.dim() == 3:\n",
    "            batched = True\n",
    "\n",
    "        if batched == True:\n",
    "            if self.training and torch.rand(1) >= CONFIG.masking_prob and CONFIG.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[2] // 5\n",
    "                )(mel_spectrogram)\n",
    "        else:\n",
    "            if self.training and torch.rand(1) >= CONFIG.masking_prob and CONFIG.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[0] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "\n",
    "\n",
    "        if batched == True:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(1)\n",
    "            mel_spectrogram = mel_spectrogram.expand(-1, 3, -1, -1)\n",
    "        else:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(0)\n",
    "            mel_spectrogram = mel_spectrogram.expand(3, -1, -1)\n",
    "\n",
    "        mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        mel_spectrogram = torch.nan_to_num(mel_spectrogram)\n",
    "        \n",
    "        return mel_spectrogram\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n",
    "    \n",
    "    \n",
    "class Mel_Classifier(nn.Module):\n",
    "    def __init__(self, mel_generator: MelSpectrogramLayer, model_name=\"tf_efficientnet_b4_ns\", embedding_size=768, pretrained=True, num_classes = 264):\n",
    "        super(Mel_Classifier, self).__init__()\n",
    "        self.mel_generator = mel_generator\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.pooling = GeM()\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "        self.fc = nn.Linear(embedding_size, num_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = self.mel_generator(images)\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        embedding = self.embedding(pooled_features)\n",
    "        output = self.fc(embedding)\n",
    "        return output\n",
    "\n",
    "# class Mel_Classifier(torch.nn.Module):\n",
    "#     def __init__(self, model_name, mel_generator: MelSpectrogramLayer, num_classes = 264, pretrained = True):\n",
    "#         super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         self.mel_generator = mel_generator\n",
    "\n",
    "#         self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "#         if 'res' in model_name:\n",
    "#             self.in_features = self.backbone.fc.in_features\n",
    "#             self.backbone.fc = nn.Linear(self.in_features, num_classes)\n",
    "#         elif 'dense' in model_name:\n",
    "#             self.in_features = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Linear(self.in_features, num_classes)\n",
    "#         elif 'efficientnet' in model_name:\n",
    "#             self.in_features = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Sequential(\n",
    "#                 nn.Linear(self.in_features, num_classes)\n",
    "#             )\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.mel_generator(x)\n",
    "#         x = self.backbone(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################3\n",
    "    \n",
    "\n",
    "# https://www.kaggle.com/code/leonshangguan/faster-eb0-sed-model-inference\n",
    "\n",
    "# def init_layer(layer):\n",
    "#     nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "#     if hasattr(layer, \"bias\"):\n",
    "#         if layer.bias is not None:\n",
    "#             layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "# def init_bn(bn):\n",
    "#     bn.bias.data.fill_(0.)\n",
    "#     bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "# def init_weights(model):\n",
    "#     classname = model.__class__.__name__\n",
    "#     if classname.find(\"Conv2d\") != -1:\n",
    "#         nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "#         model.bias.data.fill_(0)\n",
    "#     elif classname.find(\"BatchNorm\") != -1:\n",
    "#         model.weight.data.normal_(1.0, 0.02)\n",
    "#         model.bias.data.fill_(0)\n",
    "#     elif classname.find(\"GRU\") != -1:\n",
    "#         for weight in model.parameters():\n",
    "#             if len(weight.size()) > 1:\n",
    "#                 nn.init.orghogonal_(weight.data)\n",
    "#     elif classname.find(\"Linear\") != -1:\n",
    "#         model.weight.data.normal_(0, 0.01)\n",
    "#         model.bias.data.zero_()\n",
    "\n",
    "\n",
    "# def interpolate(x: torch.Tensor, ratio: int):\n",
    "#     \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "#     resolution reduction in downsampling of a CNN.\n",
    "#     Args:\n",
    "#       x: (batch_size, time_steps, classes_num)\n",
    "#       ratio: int, ratio to interpolate\n",
    "#     Returns:\n",
    "#       upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "#     \"\"\"\n",
    "#     (batch_size, time_steps, classes_num) = x.shape\n",
    "#     upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "#     upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "#     return upsampled\n",
    "\n",
    "\n",
    "# def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "#     \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "#     is the same as the value of the last frame.\n",
    "#     Args:\n",
    "#       framewise_output: (batch_size, frames_num, classes_num)\n",
    "#       frames_num: int, number of frames to pad\n",
    "#     Outputs:\n",
    "#       output: (batch_size, frames_num, classes_num)\n",
    "#     \"\"\"\n",
    "#     output = torch.nn.functional.interpolate(\n",
    "#         framewise_output.unsqueeze(1),\n",
    "#         size=(frames_num, framewise_output.size(2)),\n",
    "#         align_corners=True,\n",
    "#         mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "#     return output\n",
    "\n",
    "# class AttBlockV2(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  in_features: int,\n",
    "#                  out_features: int,\n",
    "#                  activation=\"linear\"):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.activation = activation\n",
    "#         self.att = nn.Conv1d(\n",
    "#             in_channels=in_features,\n",
    "#             out_channels=out_features,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             padding=0,\n",
    "#             bias=True)\n",
    "#         self.cla = nn.Conv1d(\n",
    "#             in_channels=in_features,\n",
    "#             out_channels=out_features,\n",
    "#             kernel_size=1,\n",
    "#             stride=1,\n",
    "#             padding=0,\n",
    "#             bias=True)\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         init_layer(self.att)\n",
    "#         init_layer(self.cla)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (n_samples, n_in, n_time)\n",
    "#         norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "#         cla = self.nonlinear_transform(self.cla(x))\n",
    "#         x = torch.sum(norm_att * cla, dim=2)\n",
    "#         return x, norm_att, cla\n",
    "\n",
    "#     def nonlinear_transform(self, x):\n",
    "#         if self.activation == 'linear':\n",
    "#             return x\n",
    "#         elif self.activation == 'sigmoid':\n",
    "#             return torch.sigmoid(x)\n",
    "\n",
    "# class Mel_Classifier(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         model_name: str,\n",
    "#         mel_generator: MelSpectrogramLayer,\n",
    "#         # config=None,\n",
    "#         pretrained=True, \n",
    "#         num_classes=264, \n",
    "#         in_channels=3\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # self.config = config\n",
    "\n",
    "#         self.mel_generator = mel_generator\n",
    "\n",
    "#         # self.bn0 = nn.BatchNorm2d(self.config.n_mels)\n",
    "\n",
    "#         base_model = timm.create_model(\n",
    "#             model_name, \n",
    "#             pretrained=pretrained, \n",
    "#             num_classes=0,\n",
    "#             global_pool=\"\",\n",
    "#             in_chans=in_channels,\n",
    "#         )\n",
    "        \n",
    "#         layers = list(base_model.children())[:-2]\n",
    "#         self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "#         in_features = base_model.num_features\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "#         self.att_block = AttBlockV2(\n",
    "#             in_features, num_classes, activation=\"linear\")\n",
    "\n",
    "#         self.init_weight()\n",
    "\n",
    "#     def init_weight(self):\n",
    "#         # init_bn(self.bn0)\n",
    "#         init_layer(self.fc1)\n",
    "        \n",
    "#     def forward(self, input_data):\n",
    "#         input_data = self.mel_generator(input_data)\n",
    "\n",
    "#         # if self.config.in_channels == 3:\n",
    "#         x = input_data\n",
    "#         # else:\n",
    "#         #     x = input_data[:, [0], :, :] # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "#         # frames_num = x.shape[2]\n",
    "\n",
    "#         # x = x.transpose(1, 3)\n",
    "#         # x = self.bn0(x)\n",
    "#         # x = x.transpose(1, 3)\n",
    "\n",
    "\n",
    "#         # x = x.transpose(2, 3)\n",
    "\n",
    "#         x = self.backbone(x)\n",
    "        \n",
    "#         # Aggregate in frequency axis\n",
    "#         x = torch.mean(x, dim=2)\n",
    "\n",
    "#         x1 = torch.nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "#         x2 = torch.nn.functional.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "#         x = x1 + x2\n",
    "\n",
    "#         x = x.transpose(1, 2)\n",
    "#         x = torch.nn.functional.relu_(self.fc1(x))\n",
    "#         x = x.transpose(1, 2)\n",
    "\n",
    "#         (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "#         output_dict = {\n",
    "#             \"clipwise_output\": clipwise_output,\n",
    "#         }\n",
    "\n",
    "#         # return output_dict\n",
    "#         return clipwise_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEF2023(torch.nn.Module):\n",
    "    def __init__(self, datapath: list, metadata_df, audio_transforms, sample_rate, period, soft_second_label, inherited_species_list = None, backgroundData = None, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Default values\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.period = period\n",
    "\n",
    "        self.backgroundData = backgroundData\n",
    "\n",
    "        self.df = metadata_df\n",
    "        self.datapath = datapath\n",
    "\n",
    "        self.soft_second_label = soft_second_label\n",
    "\n",
    "        if len(self.datapath) == 2:\n",
    "            self.audio_paths = [os.path.join(self.datapath[0],'train_audio'), os.path.join(self.datapath[1],'nocall')]\n",
    "        else:\n",
    "            self.audio_paths = [os.path.join(self.datapath[0],'train_audio')]\n",
    "\n",
    "        # Get species list\n",
    "        if inherited_species_list is None:\n",
    "            self.species = list(set(self.df['primary_label']))\n",
    "        else:\n",
    "            self.species = inherited_species_list\n",
    "\n",
    "        \n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        # length = self.df['cumulative_images'][-1]\n",
    "        length = len(list(self.df['primary_label']))\n",
    "        return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get row in df\n",
    "        dict_idx = dict(self.df.iloc[idx])\n",
    "\n",
    "        # Get labels as torch tensors\n",
    "        primary_label = torch.tensor([1 if dict_idx['primary_label'] == label else 0 for label in self.species],dtype=float)\n",
    "        secondary_label = torch.tensor([1 if label in dict_idx['secondary_labels'] else 0 for label in self.species], dtype=float)\n",
    "        combined_label = self._prepare_target(main_tgt=primary_label, sec_tgt=secondary_label)\n",
    "        dict_idx['combined_label_tensor'] = combined_label\n",
    "        dict_idx['primary_label_tensor'] = primary_label\n",
    "        dict_idx['secondary_label_tensor'] = secondary_label\n",
    "\n",
    "        # Load audio\n",
    "        if dict_idx['primary_label'] == 'nocall':\n",
    "            idx_dataset = 1\n",
    "        else:\n",
    "            idx_dataset = 0\n",
    "        ogg_file = os.path.join(self.audio_paths[idx_dataset],dict(self.df.iloc[idx])['filename'])\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        # Get clip of length self.period\n",
    "        target_audio_length = sample_rate * self.period\n",
    "        current_audio_length = len(waveform)\n",
    "        if current_audio_length >= target_audio_length:\n",
    "            start = random.randint(0,current_audio_length - target_audio_length - 1)\n",
    "            waveform_seg = waveform[start:start+target_audio_length]\n",
    "        else:\n",
    "            padding_length = target_audio_length - current_audio_length\n",
    "            waveform_seg = torch.nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=self.sample_rate)\n",
    "        waveform_seg = resampler(waveform_seg)\n",
    "\n",
    "        if random.uniform(0,1) > CONFIG.addbackground_prob and self.backgroundData is not None:\n",
    "            idx = random.randint(0,len(self.backgroundData)-1)\n",
    "            backgroundNoise = self.backgroundData[idx][0]\n",
    "            waveform_seg += backgroundNoise\n",
    "\n",
    "        return waveform_seg, combined_label\n",
    "\n",
    "    # https://github.com/VSydorskyy/BirdCLEF_2023_1st_place/blob/main/code_base/datasets/wave_dataset.py, changed\n",
    "    def _prepare_target(self, main_tgt, sec_tgt, all_labels=None):\n",
    "        all_tgt = main_tgt + sec_tgt * self.soft_second_label\n",
    "        all_tgt = torch.clamp(all_tgt, 0.0, 1.0)\n",
    "        return all_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoCallDataset(torch.nn.Module):\n",
    "    def __init__(self, datapath, metadata_df, audio_transforms, sample_rate, period, inherited_species_list=None, *args, **kwargs) -> None:\n",
    "\n",
    "        # Default values\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.period = period\n",
    "\n",
    "        self.df = metadata_df\n",
    "        self.datapath = datapath\n",
    "\n",
    "        self.soft_second_label = 0\n",
    "\n",
    "        self.audio_paths = [0,os.path.join(self.datapath[1],'nocall')]\n",
    "\n",
    "\n",
    "        # Get species list\n",
    "        if inherited_species_list is None:\n",
    "            self.species = list(set(self.df['primary_label']))\n",
    "        else:\n",
    "            self.species = inherited_species_list\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        # length = self.df['cumulative_images'][-1]\n",
    "        length = len(list(self.df['primary_label']))\n",
    "        return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get row in df\n",
    "        dict_idx = dict(self.df.iloc[idx])\n",
    "\n",
    "        # Get labels as torch tensors\n",
    "        primary_label = torch.tensor([1 if dict_idx['primary_label'] == label else 0 for label in self.species],dtype=float)\n",
    "        secondary_label = torch.tensor([1 if label in dict_idx['secondary_labels'] else 0 for label in self.species], dtype=float)\n",
    "        combined_label = self._prepare_target(main_tgt=primary_label, sec_tgt=secondary_label)\n",
    "        dict_idx['combined_label_tensor'] = combined_label\n",
    "        dict_idx['primary_label_tensor'] = primary_label\n",
    "        dict_idx['secondary_label_tensor'] = secondary_label\n",
    "\n",
    "        # Load audio\n",
    "        if dict_idx['primary_label'] == 'nocall':\n",
    "            idx_dataset = 1\n",
    "        else:\n",
    "            idx_dataset = 0\n",
    "        ogg_file = os.path.join(self.audio_paths[idx_dataset],dict(self.df.iloc[idx])['filename'])\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        # Get clip of length self.period\n",
    "        target_audio_length = sample_rate * self.period\n",
    "        current_audio_length = len(waveform)\n",
    "        if current_audio_length >= target_audio_length:\n",
    "            start = random.randint(0,current_audio_length - target_audio_length - 1)\n",
    "            waveform_seg = waveform[start:start+target_audio_length]\n",
    "        else:\n",
    "            padding_length = target_audio_length - current_audio_length\n",
    "            waveform_seg = torch.nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=self.sample_rate)\n",
    "        waveform_seg = resampler(waveform_seg)\n",
    "\n",
    "        return waveform_seg, combined_label\n",
    "\n",
    "    # https://github.com/VSydorskyy/BirdCLEF_2023_1st_place/blob/main/code_base/datasets/wave_dataset.py, changed\n",
    "    def _prepare_target(self, main_tgt, sec_tgt, all_labels=None):\n",
    "        all_tgt = main_tgt + sec_tgt * self.soft_second_label\n",
    "        all_tgt = torch.clamp(all_tgt, 0.0, 1.0)\n",
    "        return all_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/nischaydnk/split-creating-melspecs-stage-1\n",
    "def birds_stratified_split(df, target_col, test_size=0.2):\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    low_count_classes = class_counts[class_counts < 2].index.tolist() ### Birds with single counts\n",
    "\n",
    "    df['train'] = df[target_col].isin(low_count_classes)\n",
    "\n",
    "    train_df, val_df = train_test_split(df[~df['train']], test_size=test_size, stratify=df[~df['train']][target_col], random_state=42)\n",
    "\n",
    "    train_df = pd.concat([train_df, df[df['train']]], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Remove the 'valid' column\n",
    "    train_df.drop('train', axis=1, inplace=True)\n",
    "    val_df.drop('train', axis=1, inplace=True)\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uniformSampler(dataset):\n",
    "    n_data = len(dataset)\n",
    "    classes_lsit = dataset.species\n",
    "\n",
    "    count_int = [0] * len(classes_lsit)\n",
    "    nocall_count = 0\n",
    "\n",
    "    # Get class counts\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['primary_label'] == 'nocall':\n",
    "            nocall_count += 1\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['primary_label'])\n",
    "            count_int[species_index] += 1\n",
    "\n",
    "    # Calculate class weights\n",
    "    n_call = sum(count_int)\n",
    "    n_nocall = nocall_count\n",
    "    if (CONFIG.use_nocall == True) and n_nocall != 0:\n",
    "        class_weights = np.array(count_int) / n_call * (1 - CONFIG.frac_nocall)\n",
    "        nocall_weights = nocall_count / n_nocall * CONFIG.frac_nocall\n",
    "    else:\n",
    "        class_weights = np.array(count_int) / n_call * (1 - CONFIG.frac_nocall)\n",
    "\n",
    "    sample_weights = [0] * n_data\n",
    "\n",
    "    # Assign class weights to samples\n",
    "    for i in range(n_data):\n",
    "        if dataset.df.iloc[i]['primary_label'] == 'nocall':\n",
    "            sample_weights[i] = nocall_weights **  -1\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['primary_label'])\n",
    "            sample_weights[i] = class_weights[species_index] ** -1\n",
    "\n",
    "    # Normalize\n",
    "    sample_weights = sample_weights / sum(sample_weights)\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=n_data)\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_class_weights(dataset):\n",
    "    n_data = len(dataset)\n",
    "    classes_lsit = dataset.species\n",
    "\n",
    "    count_int = [0] * len(classes_lsit)\n",
    "    nocall_count = 0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['primary_label'] == 'nocall':\n",
    "            nocall_count += 1\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['primary_label'])\n",
    "            count_int[species_index] += 1\n",
    "\n",
    "    n_call = sum(count_int)\n",
    "    class_weights = (np.array(count_int) / n_call) ** -0.5   * len(classes_lsit)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "df = pd.read_csv(os.path.join(CONFIG.birdclef2023, 'train_metadata.csv'))\n",
    "\n",
    "# Get Split\n",
    "train_df, val_df = birds_stratified_split(df=df, target_col='primary_label', test_size=CONFIG.val_frac)\n",
    "\n",
    "# Get global species list\n",
    "species_list = list(set(df['primary_label']))\n",
    "\n",
    "# Initialize Datasets\n",
    "train_class_kwargs = {  'sample_rate': CONFIG.sample_rate,\n",
    "                        'n_fft': CONFIG.n_fft,\n",
    "                        'f_min': CONFIG.f_min,\n",
    "                        'f_max': CONFIG.f_max,\n",
    "                        'hop_length': CONFIG.hop_length,\n",
    "                        'n_mels': CONFIG.n_mels,\n",
    "                        'period': CONFIG.period,\n",
    "                        'device': CONFIG.device,\n",
    "                        'transform': CONFIG.train_transforms,\n",
    "                        'soft_second_label': CONFIG.soft_second_label\n",
    "                     }\n",
    "\n",
    "valid_class_kwargs = {   'sample_rate': CONFIG.sample_rate,\n",
    "                        'n_fft': CONFIG.n_fft,\n",
    "                        'f_min': CONFIG.f_min,\n",
    "                        'f_max': CONFIG.f_max,\n",
    "                        'hop_length': CONFIG.hop_length,\n",
    "                        'n_mels': CONFIG.n_mels,\n",
    "                        'period': CONFIG.period,\n",
    "                        'device': CONFIG.device,\n",
    "                        'transform': CONFIG.val_transforms\n",
    "                    }\n",
    "\n",
    "#Make No Call Dataframes\n",
    "df_nocall = pd.read_csv(os.path.join(CONFIG.birdclef2021_background_noise, 'ff1010bird_metadata_v1.csv'))\n",
    "df_train_nocall, df_valid_nocall = train_test_split(df_nocall, test_size=CONFIG.val_frac)\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_train_full = pd.concat([train_df, df_train_nocall], axis = 0)\n",
    "df_valid_full = pd.concat([val_df, df_valid_nocall], axis = 0)\n",
    "\n",
    "# Reset the index to create a new index for the concatenated DataFrame\n",
    "df_train_full.reset_index(drop=True, inplace=True)\n",
    "df_valid_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make Datasets\n",
    "train_dataset_nocall = NoCallDataset(datapath=[0,CONFIG.birdclef2021_background_noise], metadata_df=df_train_nocall, audio_transforms=CONFIG.train_transforms_audio, sample_rate=CONFIG.sample_rate, period=CONFIG.period, inherited_species_list=species_list)\n",
    "valid_dataset_nocall = NoCallDataset(datapath=[0,CONFIG.birdclef2021_background_noise], metadata_df=df_valid_nocall, audio_transforms=CONFIG.val_transforms_audio, sample_rate=CONFIG.sample_rate, period=CONFIG.period, inherited_species_list=species_list)\n",
    "\n",
    "# Make dataset\n",
    "if CONFIG.use_nocall == True:\n",
    "    train_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023, CONFIG.birdclef2021_background_noise], metadata_df=df_train_full, audio_transforms=CONFIG.train_transforms_audio ,sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list)\n",
    "    valid_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023, CONFIG.birdclef2021_background_noise], metadata_df=df_valid_full, audio_transforms=CONFIG.val_transforms_audio, sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list)\n",
    "else:\n",
    "    train_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023], metadata_df=train_df, audio_transforms=CONFIG.train_transforms_audio ,sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list, backgroundData=train_dataset_nocall)\n",
    "    valid_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023], metadata_df=val_df, audio_transforms=CONFIG.val_transforms_audio, sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list, backgroundData=valid_dataset_nocall)\n",
    "\n",
    "if CONFIG.uniform_sampler == False:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.train_batch_size, shuffle = True, pin_memory = True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.valid_batch_size, shuffle = True, pin_memory = True)\n",
    "else:\n",
    "    train_sampler = make_uniformSampler(dataset=train_dataset)\n",
    "    valid_sampler = make_uniformSampler(dataset=valid_dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.train_batch_size, pin_memory = True, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.valid_batch_size, pin_memory = True, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sampler(dataloader, epochs = 1):\n",
    "\n",
    "    species_list = dataloader.dataset.species\n",
    "    species_count = torch.tensor([0] * len(species_list))\n",
    "    total_samples = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(tqdm(dataloader, desc=\"Processing\")):\n",
    "            labels = data[1]\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "\n",
    "            label_idx = list(labels.argmax(1))\n",
    "            species_count[label_idx] += 1\n",
    "            total_samples += batch_size\n",
    "\n",
    "    return species_count.numpy(), total_samples\n",
    "\n",
    "\n",
    "# count_int_train, n_train = test_sampler(dataloader=train_loader)\n",
    "# count_int_val, n_val = test_sampler(dataloader=valid_loader)\n",
    "\n",
    "# penguin_means = {\n",
    "#     'Training Set': count_int_train / n_train,\n",
    "#     'Validation Set': count_int_val / n_val\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# x = np.arange(len(species_list))  # the label locations\n",
    "# width = 0.35  # the width of the bars\n",
    "# multiplier = 0\n",
    "\n",
    "# fig, ax = plt.subplots(layout='constrained', figsize=(24,12))\n",
    "# colors = ['crimson','midnightblue']\n",
    "# for i,(attribute, measurement) in enumerate(penguin_means.items()):\n",
    "#     offset = width * i\n",
    "#     rects = ax.bar(x + offset, measurement, width, label=attribute, color=colors[i], alpha=0.7)\n",
    "#     multiplier += 1\n",
    "\n",
    "# # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Fraction of Dataset [-]')\n",
    "# ax.set_title('Distribution of Classes in Validation and Training Data')\n",
    "\n",
    "# # ax.set_xticks(x + width / 2, classes_lsit)\n",
    "# # ax.set_xticks(x + width / 2)\n",
    "\n",
    "# ax.legend(loc='upper left', ncols=2)\n",
    "# ax.set_ylim(0, 0.1)\n",
    "# plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric Function as on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_cmap(solution, submission, padding_factor=5):\n",
    "    solution = solution.drop(['row_id'], axis=1, errors='ignore')\n",
    "    submission = submission.drop(['row_id'], axis=1, errors='ignore')\n",
    "    new_rows = []\n",
    "    for i in range(padding_factor):\n",
    "        new_rows.append([1 for i in range(len(solution.columns))])\n",
    "    new_rows = pd.DataFrame(new_rows)\n",
    "    new_rows.columns = solution.columns\n",
    "    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n",
    "    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n",
    "    score = sklearn.metrics.average_precision_score(\n",
    "        padded_solution.values,\n",
    "        padded_submission.values,\n",
    "        average='macro',\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mixup(X, y, y_pred, criterion):\n",
    "    X, y_a, y_b, lam = mixup_data(X, y, alpha=CONFIG.mixup_alpha)\n",
    "    loss_mixup = mixup_criterion(criterion, y_pred, y_a, y_b, lam) #cross_entropy\n",
    "    return loss_mixup\n",
    "\n",
    "def train_net(net, trainloader, valloader, criterion, optimizer, scheduler, epochs=1, patience = 3, savePth = 'project2_weights.pth', print_every_samples = 20, device = 'cpu'):\n",
    "\n",
    "    print('Using device: {}'.format(device))\n",
    "    net.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    if CONFIG.softmax_prob == True:\n",
    "        toProb = torch.nn.Softmax(dim=1)\n",
    "    else:\n",
    "        toProb = torch.nn.Identity()\n",
    "\n",
    "    validation_loss_list = [0] * epochs\n",
    "    training_loss_list = [0] * epochs\n",
    "    validation_accuracy_list = [0] * epochs\n",
    "    training_accuracy_list = [0] * epochs\n",
    "    cmap_5_list = [0] * epochs\n",
    "\n",
    "    best_state_dictionary = None\n",
    "    best_validation_cmap = 0.0\n",
    "    inertia = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        training_accuracy = 0.0\n",
    "        running_loss = 0.0\n",
    "        # Set model to training mode\n",
    "        net.mel_generator.transform = CONFIG.train_transforms\n",
    "        net = net.train()\n",
    "\n",
    "        # Calculate the number of batches to loop over\n",
    "        num_batches_to_loop = int(CONFIG.training_data_per_epoch * len(trainloader))\n",
    "        with tqdm(enumerate(trainloader, 0), total=num_batches_to_loop, desc=\"Training Batches Epoch {} / {}\".format(epoch + 1, epochs)) as train_pbar:\n",
    "            for i, data in train_pbar:\n",
    "        \n",
    "                # get the inputs\n",
    "                if device == 'cuda':\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                else:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                if CONFIG.use_mixup:\n",
    "                    loss_value = train_with_mixup(inputs, labels, outputs, criterion=criterion)\n",
    "                else:\n",
    "                    loss_value = criterion(outputs,labels)\n",
    "\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics and write to log\n",
    "                running_loss += loss_value.item()\n",
    "                training_loss += loss_value.item()\n",
    "\n",
    "                train_pbar.set_postfix(loss=(running_loss / ((i + 1) * trainloader.batch_size)))\n",
    "                training_accuracy += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "\n",
    "                if type(scheduler).__name__ != 'NoneType':\n",
    "                    scheduler.step(epoch + i / len(trainloader))\n",
    "\n",
    "                if i >= num_batches_to_loop:\n",
    "                    break\n",
    "\n",
    "        training_loss = training_loss / (len(trainloader.dataset) * CONFIG.training_data_per_epoch)\n",
    "        training_loss_list[epoch] = training_loss\n",
    "        training_accuracy = 100 * training_accuracy / (len(trainloader.dataset) * CONFIG.training_data_per_epoch)\n",
    "        training_accuracy_list[epoch] = training_accuracy\n",
    "\n",
    "        print('Batch {:5d} / {:5d}: Training Loss = {:.3f}, Training Accuracy = {:.3f}'.format(epoch + 1, epochs, training_loss, training_accuracy))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        predictions_array = np.zeros((len(valloader.dataset), len(valloader.dataset.species)), dtype=float)\n",
    "        solutions_array = np.zeros((len(valloader.dataset), len(valloader.dataset.species)), dtype=float)\n",
    "        # Set model to validation mode\n",
    "        net.mel_generator.transform = CONFIG.val_transforms\n",
    "        net = net.eval()\n",
    "        with tqdm(enumerate(valloader, 0), total=len(valloader), desc=\"Validation Batches Epoch {} / {}\".format(epoch + 1, epochs)) as val_pbar:\n",
    "            for i, data in val_pbar:\n",
    "                # get the inputs\n",
    "                if device == 'cuda':\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                else:\n",
    "                    inputs, labels = data\n",
    "                    \n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss_value = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics and write to log\n",
    "                running_loss += loss_value.item()\n",
    "                val_loss += loss_value.item()\n",
    "\n",
    "                # Get model output and label to array\n",
    "                curr_predictions_array = toProb(outputs).detach().cpu().numpy()\n",
    "                predictions_array[i*valloader.batch_size:(i+1)*valloader.batch_size,:] = curr_predictions_array\n",
    "                hardlabels = labels.detach().cpu().numpy()\n",
    "                hardlabels[hardlabels < 0.99] = 0\n",
    "                curr_solutions_array = hardlabels\n",
    "                solutions_array[i*valloader.batch_size:(i+1)*valloader.batch_size,:] = curr_solutions_array\n",
    "\n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix(loss=(running_loss / ((i + 1) * trainloader.batch_size)))\n",
    "                correct += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "        \n",
    "        # Get cMAP\n",
    "        cmap_5 = padded_cmap(solution=pd.DataFrame(solutions_array), submission=pd.DataFrame(predictions_array), padding_factor=5)\n",
    "\n",
    "        # Get Metrics\n",
    "        val_loss = val_loss / len(valloader.dataset)\n",
    "        validation_loss_list[epoch] = val_loss\n",
    "        val_accuracy = 100 * correct / len(valloader.dataset)\n",
    "        validation_accuracy_list[epoch] = val_accuracy\n",
    "        cmap_5_list[epoch] = cmap_5\n",
    "\n",
    "        print('Batch {:5d} / {:5d}: Validation Loss = {:.3f}, Validation Accuracy = {:.3f}, cmap score = {:.3f}'.format(epoch + 1, epochs, val_loss, val_accuracy, cmap_5))\n",
    "\n",
    "        save_weights = os.path.join(savePth,'model_weights.pth')\n",
    "        if cmap_5 > best_validation_cmap:\n",
    "            best_validation_cmap = cmap_5\n",
    "            best_state_dictionary = copy.deepcopy(net.state_dict())\n",
    "            # save network\n",
    "            torch.save(best_state_dictionary, save_weights)\n",
    "            inertia = 0\n",
    "            print('Epoch {:5d} / {:5d} saved: New Best Epoch!'.format(epoch + 1, epochs))\n",
    "        else:\n",
    "            inertia += 1\n",
    "            if inertia == patience:\n",
    "                if best_state_dictionary is None:\n",
    "                    raise Exception(\"State dictionary should have been updated at least once\")\n",
    "                break\n",
    "        # print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    output = {'validation_loss': validation_loss_list,\n",
    "              'validation_accuracy': validation_accuracy_list,\n",
    "              'training_loss': training_loss_list,\n",
    "              'training_accuracy': training_accuracy_list,\n",
    "              'cmap_5_scores': cmap_5_list}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    melspec_layer = MelSpectrogramLayer(sample_rate=config.sample_rate,\n",
    "                                    n_fft=config.n_fft,\n",
    "                                    hop_length=config.hop_length,\n",
    "                                    n_mels=config.n_mels,\n",
    "                                    transform=config.train_transforms)\n",
    "    network = Mel_Classifier(model_name=config.model_name,\n",
    "                            mel_generator=melspec_layer)\n",
    "    \n",
    "    if config.load_pretrained_weights == True:\n",
    "        print('Load PreTrained Weigths')\n",
    "        network.load_state_dict(torch.load(config.pretrained_weights, map_location=config.device))\n",
    "\n",
    "    if config.fix_features == True:\n",
    "        for param in network.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return network\n",
    "\n",
    "def get_criterion(config):\n",
    "    \n",
    "    if config.criterion == 'LabelSmoothingBCEWithLogitsLoss':\n",
    "        if config.class_weighting == True:\n",
    "            class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "            criterion = LabelSmoothingBCEWithLogitsLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = LabelSmoothingBCEWithLogitsLoss()\n",
    "\n",
    "    if config.criterion == 'CrossEntropyLoss':\n",
    "        if config.class_weighting == True:\n",
    "            class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if config.criterion == 'BCEWithLogitsLoss':\n",
    "        if config.class_weighting == True:\n",
    "            class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "            criterion = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "        else:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return criterion\n",
    "\n",
    "def get_optimizer(config, params):\n",
    "\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, params), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, params), lr=config.lr, momentum=config.momentum)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(config, optimizer):\n",
    "\n",
    "    if config.scheduler == 'cosineannealing':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                            optimizer, \n",
    "                            T_0=config.epochs, \n",
    "                            T_mult=config.T_mult, \n",
    "                            eta_min=config.eta_min, \n",
    "                            last_epoch=config.last_epoch\n",
    "                        )\n",
    "    elif config.scheduler == None:\n",
    "        config.scheduler = None\n",
    "\n",
    "    return scheduler\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train():\n",
    "\n",
    "    # Change Output path\n",
    "    folder_name = time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    outpath = os.path.join(CONFIG.outpath, folder_name)\n",
    "    CONFIG.outpath = outpath\n",
    "\n",
    "    # Create Output directory\n",
    "    os.makedirs(CONFIG.outpath, exist_ok=True)\n",
    "\n",
    "    # Get all variables to json file\n",
    "    config_dict = {attr: value for attr, value in vars(CONFIG).items()}\n",
    "    outputName = 'hyperparameters.json'\n",
    "    jsonpath = os.path.join(CONFIG.outpath, outputName)\n",
    "    with open(jsonpath, 'w') as json_file:\n",
    "        json.dump(str(config_dict), json_file)\n",
    "\n",
    "    # Get model\n",
    "    network = get_model(config = CONFIG)\n",
    "    \n",
    "    # Get loss function\n",
    "    criterion = get_criterion(config = CONFIG)\n",
    "    \n",
    "    # Get optimizer\n",
    "    optimizer = get_optimizer(config = CONFIG, params = network.parameters())\n",
    "\n",
    "    # Get scheduler\n",
    "    scheduler = get_scheduler(config = CONFIG, optimizer = optimizer)\n",
    "\n",
    "    # Train Net\n",
    "    output = train_net( net=network,\n",
    "                        trainloader=train_loader,\n",
    "                        valloader=valid_loader,\n",
    "                        criterion=criterion,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        epochs=CONFIG.epochs,\n",
    "                        device=CONFIG.device,\n",
    "                        print_every_samples=CONFIG.print_every_n_batches,\n",
    "                        savePth=CONFIG.outpath,\n",
    "                        patience=CONFIG.patience\n",
    "                        )\n",
    "    \n",
    "    # Save Output\n",
    "    outputName = 'training_prog.json'\n",
    "    jsonpath = os.path.join(CONFIG.outpath, outputName)\n",
    "    with open(jsonpath, 'w') as json_file:\n",
    "        json.dump(output, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches Epoch 1 / 25: 100%|██████████| 59/59 [10:30<00:00, 10.69s/it, loss=0.00114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch     1 /    25: Training Loss = 0.001, Training Accuracy = 0.341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Batches Epoch 1 / 25:   0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle: Zip Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# https://www.kaggle.com/code/hari31416/downloading-file-and-directory-from-kaggle\n",
    "def zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n",
    "    \"\"\"\n",
    "    zip all the files in a directory\n",
    "    \n",
    "    Parameters\n",
    "    _____\n",
    "    directory: str\n",
    "        directory needs to be zipped, defualt is current working directory\n",
    "        \n",
    "    file_name: str\n",
    "        the name of the zipped file (including .zip), default is 'directory.zip'\n",
    "        \n",
    "    Returns\n",
    "    _____\n",
    "    Creates a hyperlink, which can be used to download the zip file)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)\n",
    "    zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file_name in file:\n",
    "                pass\n",
    "            else:\n",
    "                zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "    return FileLink(file_name)\n",
    "\n",
    "if CONFIG.run_environment == 'kaggle':\n",
    "    zip_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix issue that not all labels are in validation set\n",
    "# TODO: Fix progress bar\n",
    "# TODO: adam?\n",
    "# TODO: cMAP\n",
    "# TODO: Train with mixup    \n",
    "# TODO: Dataset metrics\n",
    "# TODO: Uniform sampler accross classes\n",
    "# TODO: class weights\n",
    "# TODO: use only sub-dataset per epoch?\n",
    "# TODO: consider second label\n",
    "# TODO: Inference Script\n",
    "\n",
    "# TODO: add no-call samples\n",
    "# TODO: augmentations\n",
    "# TODO: include rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try class weights by 1st rank:\n",
    "sample_weights = (\n",
    "    all_primary_labels.value_counts() / \n",
    "    all_primary_labels.value_counts().sum()\n",
    ")  ** (-0.5)\n",
    "\n",
    "Also by 1st rank:\n",
    "\n",
    "Small inference tricks\n",
    "\n",
    "    Using temperature mean: pred = (pred**2).mean(axis=0) ** 0.5\n",
    "    Using Attention SED probs * 0.75 + Max Timewise probs * 0.25\n",
    "\n",
    "All these gave marginal improvements but it is was a matter of first 3 places :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
