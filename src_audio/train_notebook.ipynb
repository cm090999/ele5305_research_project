{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook for BirdCLEF2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all Dependencies (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import bisect\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CONFIG class containing all relevant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, smooth_eps=0.0025, weight=None, reduction=\"mean\"):\n",
    "        super(LabelSmoothingBCEWithLogitsLoss, self).__init__()\n",
    "        self.smooth_eps = smooth_eps\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.bce_with_logits_loss = nn.BCEWithLogitsLoss(weight=self.weight, reduction=self.reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target_smooth = torch.clamp(target.float(), self.smooth_eps, 1.0 - self.smooth_eps)\n",
    "        target_smooth = target_smooth + (self.smooth_eps / target.size(1))\n",
    "        return self.bce_with_logits_loss(input, target_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE UNLESS RE-GENERATING DATASET IMAGES ###\n",
    "class Config_Mel():\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        # Device\n",
    "        self.device = 'cpu'\n",
    "        \n",
    "        # Dataset Path\n",
    "        self.birdclef2023 = 'birdclef-2023'\n",
    "\n",
    "        # Out path\n",
    "        self.outpath_images = self.birdclef2023 + '_MelSpectrograms'\n",
    "\n",
    "        self.melSpecTransform = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        # Audio Features\n",
    "        self.sample_rate = 32000\n",
    "        self.n_fft=2048\n",
    "        self.f_min=40\n",
    "        self.f_max=15000\n",
    "        self.hop_length=512\n",
    "        self.n_mels=128\n",
    "        self.mel_args = {'sample_rate': self.sample_rate,\n",
    "                         'n_fft': self.n_fft,\n",
    "                         'f_min': self.f_min,\n",
    "                         'f_max': self.f_max,\n",
    "                         'hop_length': self.hop_length,\n",
    "                         'n_mels': self.n_mels}\n",
    "### DO NOT CHANGE UNLESS RE-GENERATING DATASET IMAGES ###\n",
    "\n",
    "class Config():\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.run_environment = 'local' # 'kaggle' 'local' 'colab'\n",
    "        self.load_pretrained_weights = False\n",
    "        self.rerun_split = False\n",
    "        self.use_5_second_dataset = True\n",
    "        self.uniform_sampler = True\n",
    "        self.training_data_per_epoch = 0.25\n",
    "        self.soft_second_label = 0.3\n",
    "        self.class_weighting = True\n",
    "        self.softmax_prob = True\n",
    "        self.use_mixup = True\n",
    "        self.criterion = LabelSmoothingBCEWithLogitsLoss # nn.CrossEntropyLoss # nn.BCEWithLogitsLoss\n",
    "        self.epochs = 25\n",
    "        self.masking = True\n",
    "        self.masking_prob = 0.5\n",
    "        self.frac_nocall = 0.3\n",
    "        self.use_nocall = False\n",
    "        self.addbackground_prob = 0.5\n",
    "\n",
    "        # Device\n",
    "        if (self.run_environment == 'kaggle') or (self.run_environment == 'colab'):\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        # Dataset Path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.birdclef2023_melspectrograms = '/kaggle/input/birdclef-2023-melspectrograms/birdclef-2023_MelSpectrograms'\n",
    "            self.birdclef2023_melspectrograms_5_seconds = '/kaggle/input/birdclef-2023-melspectrograms-5-seconds/birdclef-2023_MelSpectrograms_5_seconds'\n",
    "            self.birdclef2023 = '/kaggle/input/birdclef-2023'\n",
    "            self.birdclef2021_background_noise = '/kaggle/input/birdclef2021-background-noise/ff1010bird_nocall'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.birdclef2023_melspectrograms = '/home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms'\n",
    "            self.birdclef2023_melspectrograms_5_seconds = '/home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms_5_seconds'\n",
    "            self.birdclef2023 = '/home/colin/elec5305/ele5305_research_project/birdclef-2023'\n",
    "            self.birdclef2021_background_noise = '/home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall'\n",
    "        elif self.run_environment == 'colab':\n",
    "            self.birdclef2023_melspectrograms = ''\n",
    "\n",
    "        # Out path\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.outpath = '/kaggle/working/results'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.outpath = 'results'\n",
    "        elif self.run_environment == 'colab':\n",
    "            self.outpath = ''\n",
    "\n",
    "        # Train/Validation Split \n",
    "        self.val_frac = 0.1\n",
    "\n",
    "        # Dataloader options\n",
    "        self.num_workers = 2\n",
    "        self.train_batch_size = 64\n",
    "        self.valid_batch_size = 32\n",
    "\n",
    "        # Model name\n",
    "        self.model_name = 'tf_efficientnet_b0_ns'\n",
    "        # Pretrained\n",
    "        if self.run_environment == 'kaggle':\n",
    "            self.pretrained_weights = '/kaggle/input/weights12/model_weights.pth'\n",
    "        elif self.run_environment == 'local':\n",
    "            self.pretrained_weights = '/home/colin/elec5305/ele5305_research_project/weights/model_weights.pth'\n",
    "\n",
    "\n",
    "        # Optimizer Settings\n",
    "        self.lr=5e-4\n",
    "        self.weight_decay = 1e-3\n",
    "        self.momentum=0.9\n",
    "        self.optimizer = 'adam' # 'adam', 'sgd'\n",
    "\n",
    "        self.scheduler = 'cosineannealing'\n",
    "        self.eta_min = 1e-6\n",
    "        self.T_mult = 1\n",
    "        self.last_epoch = -1\n",
    "        self.use_amp = False\n",
    "\n",
    "        self.mixup_alpha = 0.5\n",
    "\n",
    "        # Training Settings\n",
    "        self.print_every_n_batches = 25\n",
    "        self.patience = 5\n",
    "        self.fix_features = False\n",
    "\n",
    "        # Image Transforms\n",
    "        self.train_transforms = torchvision.transforms.Compose([\n",
    "                    torchaudio.transforms.AmplitudeToDB(),\n",
    "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.RandomResizedCrop(size=(128, 312), scale = (0.75, 1.0), antialias=True), \n",
    "                    ])\n",
    "        \n",
    "        self.val_transforms = torchvision.transforms.Compose([\n",
    "                    torchaudio.transforms.AmplitudeToDB(),\n",
    "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    torchvision.transforms.Resize(size=[128,312], antialias=True),\n",
    "                    # torchvision.transforms.RandomResizedCrop(size=(128, 312), scale = (0.75, 1.0), antialias=True), \n",
    "                    ])\n",
    "        \n",
    "        self.test_transforms = torchvision.transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    # torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    # torchvision.transforms.Resize(size=(224, 224), antialias=True),  # Or Resize(antialias=True)\n",
    "                    ])\n",
    "\n",
    "        # Audio Transforms\n",
    "        self.train_transforms_audio = None\n",
    "        \n",
    "        self.val_transforms_audio = None\n",
    "        \n",
    "        self.test_transforms_audio = None\n",
    "\n",
    "\n",
    "        # Audio Features\n",
    "        self.sample_rate = 32000\n",
    "        self.period = 5\n",
    "\n",
    "        # Mel Spectrogram Parameters\n",
    "        self.n_fft=2048\n",
    "        self.f_min=40\n",
    "        self.f_max=15000\n",
    "        self.hop_length=512\n",
    "        self.n_mels=128\n",
    "        self.mel_args = {'n_fft': self.n_fft,\n",
    "                         'f_min': self.f_min,\n",
    "                         'f_max': self.f_max,\n",
    "                         'hop_length': self.hop_length,\n",
    "                         'n_mels': self.n_mels}\n",
    "        \n",
    "        if self.run_environment == 'colab':\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "\n",
    "            notebook_path = 'My Drive/elec5305'\n",
    "\n",
    "            env_path = f'/content/drive/{notebook_path}'\n",
    "            # Add the handout folder to python paths\n",
    "            if env_path not in sys.path:\n",
    "                sys.path.append(env_path)\n",
    "\n",
    "            # zip_path = os.path.join(env_path, 'birdclef-2023_MelSpectrograms.zip')\n",
    "            zip_path = '/content/drive/MyDrive/elec5305/birdclef-2023_MelSpectrograms.zip'\n",
    "            shutil.unpack_archive(zip_path, \"content/\")\n",
    "            print(zip_path)\n",
    "            # !unzip zip_path -d \"/content\"\n",
    "\n",
    "            # Dataset path\n",
    "            self.birdclef2023_melspectrograms = '/content/content/birdclef-2023_MelSpectrograms'\n",
    "\n",
    "            # Output path\n",
    "            self.outpath = os.path.join(env_path, 'results')\n",
    "            os.makedirs(self.outpath, exist_ok=True)# !pip install --force-reinstall numpy==1.22.1\n",
    "            \n",
    "            %pip install -q torchtoolbox timm\n",
    "\n",
    "            %pip install timm torchtoolbox\n",
    "\n",
    "        if CONFIG.run_environment != 'local':\n",
    "            # !pip install --force-reinstall numpy==1.22.1\n",
    "            %pip install -q torchtoolbox timm\n",
    "        \n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all dependencies (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtoolbox.tools import mixup_data, mixup_criterion\n",
    "from torch.nn.functional import cross_entropy\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mel_Classifier(torch.nn.Module):\n",
    "#     def __init__(self, model_name, mel_generator: MelSpectrogramLayer, num_classes = 264, pretrained = True):\n",
    "#         super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         self.mel_generator = mel_generator\n",
    "\n",
    "#         self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "#         if 'res' in model_name:\n",
    "#             self.in_features = self.backbone.fc.in_features\n",
    "#             self.backbone.fc = nn.Linear(self.in_features, num_classes)\n",
    "#         elif 'dense' in model_name:\n",
    "#             self.in_features = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Linear(self.in_features, num_classes)\n",
    "#         elif 'efficientnet' in model_name:\n",
    "#             self.in_features = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Sequential(\n",
    "#                 nn.Linear(self.in_features, num_classes)\n",
    "#             )\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.mel_generator(x)\n",
    "#         x = self.backbone(x)\n",
    "#         return x\n",
    "\n",
    "class MelSpectrogramLayer(nn.Module):\n",
    "    def __init__(self, sample_rate, n_fft, hop_length, n_mels, transform):\n",
    "        super(MelSpectrogramLayer, self).__init__()\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        mel_spectrogram = self.mel_transform(waveform)\n",
    "\n",
    "        batched = True\n",
    "        if mel_spectrogram.dim() == 2:\n",
    "            batched = False\n",
    "        elif mel_spectrogram.dim() == 3:\n",
    "            batched = True\n",
    "\n",
    "        if batched == True:\n",
    "            if self.training and torch.rand(1) >= CONFIG.masking_prob and CONFIG.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[2] // 5\n",
    "                )(mel_spectrogram)\n",
    "        else:\n",
    "            if self.training and torch.rand(1) >= CONFIG.masking_prob and CONFIG.masking == True:\n",
    "                mel_spectrogram = torchaudio.transforms.FrequencyMasking(\n",
    "                    freq_mask_param=mel_spectrogram.shape[0] // 5\n",
    "                )(mel_spectrogram)\n",
    "                mel_spectrogram = torchaudio.transforms.TimeMasking(\n",
    "                    time_mask_param=mel_spectrogram.shape[1] // 5\n",
    "                )(mel_spectrogram)\n",
    "\n",
    "\n",
    "        if batched == True:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(1)\n",
    "            mel_spectrogram = mel_spectrogram.expand(-1, 3, -1, -1)\n",
    "        else:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(0)\n",
    "            mel_spectrogram = mel_spectrogram.expand(3, -1, -1)\n",
    "\n",
    "        mel_spectrogram = self.transform(mel_spectrogram)\n",
    "\n",
    "        mel_spectrogram = torch.nan_to_num(mel_spectrogram)\n",
    "        \n",
    "        return mel_spectrogram\n",
    "    \n",
    "\n",
    "# https://www.kaggle.com/code/leonshangguan/faster-eb0-sed-model-inference\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = torch.nn.functional.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class Mel_Classifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        mel_generator: MelSpectrogramLayer,\n",
    "        # config=None,\n",
    "        pretrained=True, \n",
    "        num_classes=264, \n",
    "        in_channels=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.config = config\n",
    "\n",
    "        self.mel_generator = mel_generator\n",
    "\n",
    "        # self.bn0 = nn.BatchNorm2d(self.config.n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=0,\n",
    "            global_pool=\"\",\n",
    "            in_chans=in_channels,\n",
    "        )\n",
    "        \n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"linear\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        # init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        input_data = self.mel_generator(input_data)\n",
    "\n",
    "        # if self.config.in_channels == 3:\n",
    "        x = input_data\n",
    "        # else:\n",
    "        #     x = input_data[:, [0], :, :] # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        # frames_num = x.shape[2]\n",
    "\n",
    "        # x = x.transpose(1, 3)\n",
    "        # x = self.bn0(x)\n",
    "        # x = x.transpose(1, 3)\n",
    "\n",
    "\n",
    "        # x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = torch.nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = torch.nn.functional.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.nn.functional.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "        output_dict = {\n",
    "            \"clipwise_output\": clipwise_output,\n",
    "        }\n",
    "\n",
    "        # return output_dict\n",
    "        return clipwise_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEF2023(torch.nn.Module):\n",
    "    def __init__(self, datapath: list, metadata_df, audio_transforms, sample_rate, period, soft_second_label, inherited_species_list = None, backgroundData = None, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Default values\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.period = period\n",
    "\n",
    "        self.backgroundData = backgroundData\n",
    "\n",
    "        self.df = metadata_df\n",
    "        self.datapath = datapath\n",
    "\n",
    "        self.soft_second_label = soft_second_label\n",
    "\n",
    "        if len(self.datapath) == 2:\n",
    "            self.audio_paths = [os.path.join(self.datapath[0],'train_audio'), os.path.join(self.datapath[1],'nocall')]\n",
    "        else:\n",
    "            self.audio_paths = [os.path.join(self.datapath[0],'train_audio')]\n",
    "\n",
    "        # Get species list\n",
    "        if inherited_species_list is None:\n",
    "            self.species = list(set(self.df['primary_label']))\n",
    "        else:\n",
    "            self.species = inherited_species_list\n",
    "\n",
    "        \n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        # length = self.df['cumulative_images'][-1]\n",
    "        length = len(list(self.df['primary_label']))\n",
    "        return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get row in df\n",
    "        dict_idx = dict(self.df.iloc[idx])\n",
    "\n",
    "        # Get labels as torch tensors\n",
    "        primary_label = torch.tensor([1 if dict_idx['primary_label'] == label else 0 for label in self.species],dtype=float)\n",
    "        secondary_label = torch.tensor([1 if label in dict_idx['secondary_labels'] else 0 for label in self.species], dtype=float)\n",
    "        combined_label = self._prepare_target(main_tgt=primary_label, sec_tgt=secondary_label)\n",
    "        dict_idx['combined_label_tensor'] = combined_label\n",
    "        dict_idx['primary_label_tensor'] = primary_label\n",
    "        dict_idx['secondary_label_tensor'] = secondary_label\n",
    "\n",
    "        # Load audio\n",
    "        if dict_idx['primary_label'] == 'nocall':\n",
    "            idx_dataset = 1\n",
    "        else:\n",
    "            idx_dataset = 0\n",
    "        ogg_file = os.path.join(self.audio_paths[idx_dataset],dict(self.df.iloc[idx])['filename'])\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        # Get clip of length self.period\n",
    "        target_audio_length = sample_rate * self.period\n",
    "        current_audio_length = len(waveform)\n",
    "        if current_audio_length >= target_audio_length:\n",
    "            start = random.randint(0,current_audio_length - target_audio_length - 1)\n",
    "            waveform_seg = waveform[start:start+target_audio_length]\n",
    "        else:\n",
    "            padding_length = target_audio_length - current_audio_length\n",
    "            waveform_seg = torch.nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=self.sample_rate)\n",
    "        waveform_seg = resampler(waveform_seg)\n",
    "\n",
    "        if random.uniform(0,1) > CONFIG.addbackground_prob and self.backgroundData is not None:\n",
    "            idx = random.randint(0,len(self.backgroundData)-1)\n",
    "            backgroundNoise = self.backgroundData[idx][0]\n",
    "            waveform_seg += backgroundNoise\n",
    "\n",
    "        return waveform_seg, combined_label\n",
    "\n",
    "    # https://github.com/VSydorskyy/BirdCLEF_2023_1st_place/blob/main/code_base/datasets/wave_dataset.py, changed\n",
    "    def _prepare_target(self, main_tgt, sec_tgt, all_labels=None):\n",
    "        all_tgt = main_tgt + sec_tgt * self.soft_second_label\n",
    "        all_tgt = torch.clamp(all_tgt, 0.0, 1.0)\n",
    "        return all_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoCallDataset(torch.nn.Module):\n",
    "    def __init__(self, datapath, metadata_df, audio_transforms, sample_rate, period, inherited_species_list=None, *args, **kwargs) -> None:\n",
    "\n",
    "        # Default values\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.period = period\n",
    "\n",
    "        self.df = metadata_df\n",
    "        self.datapath = datapath\n",
    "\n",
    "        self.soft_second_label = 0\n",
    "\n",
    "        self.audio_paths = [0,os.path.join(self.datapath[1],'nocall')]\n",
    "\n",
    "\n",
    "        # Get species list\n",
    "        if inherited_species_list is None:\n",
    "            self.species = list(set(self.df['primary_label']))\n",
    "        else:\n",
    "            self.species = inherited_species_list\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        # length = self.df['cumulative_images'][-1]\n",
    "        length = len(list(self.df['primary_label']))\n",
    "        return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get row in df\n",
    "        dict_idx = dict(self.df.iloc[idx])\n",
    "\n",
    "        # Get labels as torch tensors\n",
    "        primary_label = torch.tensor([1 if dict_idx['primary_label'] == label else 0 for label in self.species],dtype=float)\n",
    "        secondary_label = torch.tensor([1 if label in dict_idx['secondary_labels'] else 0 for label in self.species], dtype=float)\n",
    "        combined_label = self._prepare_target(main_tgt=primary_label, sec_tgt=secondary_label)\n",
    "        dict_idx['combined_label_tensor'] = combined_label\n",
    "        dict_idx['primary_label_tensor'] = primary_label\n",
    "        dict_idx['secondary_label_tensor'] = secondary_label\n",
    "\n",
    "        # Load audio\n",
    "        if dict_idx['primary_label'] == 'nocall':\n",
    "            idx_dataset = 1\n",
    "        else:\n",
    "            idx_dataset = 0\n",
    "        ogg_file = os.path.join(self.audio_paths[idx_dataset],dict(self.df.iloc[idx])['filename'])\n",
    "        waveform, sample_rate = torchaudio.load(ogg_file)\n",
    "        waveform = waveform.ravel()\n",
    "\n",
    "        # Get clip of length self.period\n",
    "        target_audio_length = sample_rate * self.period\n",
    "        current_audio_length = len(waveform)\n",
    "        if current_audio_length >= target_audio_length:\n",
    "            start = random.randint(0,current_audio_length - target_audio_length - 1)\n",
    "            waveform_seg = waveform[start:start+target_audio_length]\n",
    "        else:\n",
    "            padding_length = target_audio_length - current_audio_length\n",
    "            waveform_seg = torch.nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=self.sample_rate)\n",
    "        waveform_seg = resampler(waveform_seg)\n",
    "\n",
    "        return waveform_seg, combined_label\n",
    "\n",
    "    # https://github.com/VSydorskyy/BirdCLEF_2023_1st_place/blob/main/code_base/datasets/wave_dataset.py, changed\n",
    "    def _prepare_target(self, main_tgt, sec_tgt, all_labels=None):\n",
    "        all_tgt = main_tgt + sec_tgt * self.soft_second_label\n",
    "        all_tgt = torch.clamp(all_tgt, 0.0, 1.0)\n",
    "        return all_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/nischaydnk/split-creating-melspecs-stage-1\n",
    "def birds_stratified_split(df, target_col, test_size=0.2):\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    low_count_classes = class_counts[class_counts < 2].index.tolist() ### Birds with single counts\n",
    "\n",
    "    df['train'] = df[target_col].isin(low_count_classes)\n",
    "\n",
    "    train_df, val_df = train_test_split(df[~df['train']], test_size=test_size, stratify=df[~df['train']][target_col], random_state=42)\n",
    "\n",
    "    train_df = pd.concat([train_df, df[df['train']]], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Remove the 'valid' column\n",
    "    train_df.drop('train', axis=1, inplace=True)\n",
    "    val_df.drop('train', axis=1, inplace=True)\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uniformSampler(dataset):\n",
    "    n_data = len(dataset)\n",
    "    classes_lsit = dataset.species\n",
    "\n",
    "    count_int = [0] * len(classes_lsit)\n",
    "    nocall_count = 0\n",
    "\n",
    "    # Get class counts\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['primary_label'] == 'nocall':\n",
    "            nocall_count += 1\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['primary_label'])\n",
    "            count_int[species_index] += 1\n",
    "\n",
    "    # Calculate class weights\n",
    "    n_call = sum(count_int)\n",
    "    n_nocall = nocall_count\n",
    "    if (CONFIG.use_nocall == True) and n_nocall != 0:\n",
    "        class_weights = np.array(count_int) / n_call * (1 - CONFIG.frac_nocall)\n",
    "        nocall_weights = nocall_count / n_nocall * CONFIG.frac_nocall\n",
    "    else:\n",
    "        class_weights = np.array(count_int) / n_call * (1 - CONFIG.frac_nocall)\n",
    "\n",
    "    sample_weights = [0] * n_data\n",
    "\n",
    "    # Assign class weights to samples\n",
    "    for i in range(n_data):\n",
    "        if dataset.df.iloc[i]['primary_label'] == 'nocall':\n",
    "            sample_weights[i] = nocall_weights **  -1\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['primary_label'])\n",
    "            sample_weights[i] = class_weights[species_index] ** -1\n",
    "\n",
    "    # Normalize\n",
    "    sample_weights = sample_weights / sum(sample_weights)\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=n_data)\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_class_weights(dataset):\n",
    "    n_data = len(dataset)\n",
    "    classes_lsit = dataset.species\n",
    "\n",
    "    count_int = [0] * len(classes_lsit)\n",
    "    nocall_count = 0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset.df.iloc[i]['primary_label'] == 'nocall':\n",
    "            nocall_count += 1\n",
    "        else:\n",
    "            species_index = classes_lsit.index(dataset.df.iloc[i]['primary_label'])\n",
    "            count_int[species_index] += 1\n",
    "\n",
    "    n_call = sum(count_int)\n",
    "    class_weights = (np.array(count_int) / n_call) ** -0.5   * len(classes_lsit)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "df = pd.read_csv(os.path.join(CONFIG.birdclef2023, 'train_metadata.csv'))\n",
    "\n",
    "# Get Split\n",
    "train_df, val_df = birds_stratified_split(df=df, target_col='primary_label', test_size=CONFIG.val_frac)\n",
    "\n",
    "# Get global species list\n",
    "species_list = list(set(df['primary_label']))\n",
    "\n",
    "# Initialize Datasets\n",
    "train_class_kwargs = {  'sample_rate': CONFIG.sample_rate,\n",
    "                        'n_fft': CONFIG.n_fft,\n",
    "                        'f_min': CONFIG.f_min,\n",
    "                        'f_max': CONFIG.f_max,\n",
    "                        'hop_length': CONFIG.hop_length,\n",
    "                        'n_mels': CONFIG.n_mels,\n",
    "                        'period': CONFIG.period,\n",
    "                        'device': CONFIG.device,\n",
    "                        'transform': CONFIG.train_transforms,\n",
    "                        'soft_second_label': CONFIG.soft_second_label\n",
    "                     }\n",
    "\n",
    "valid_class_kwargs = {   'sample_rate': CONFIG.sample_rate,\n",
    "                        'n_fft': CONFIG.n_fft,\n",
    "                        'f_min': CONFIG.f_min,\n",
    "                        'f_max': CONFIG.f_max,\n",
    "                        'hop_length': CONFIG.hop_length,\n",
    "                        'n_mels': CONFIG.n_mels,\n",
    "                        'period': CONFIG.period,\n",
    "                        'device': CONFIG.device,\n",
    "                        'transform': CONFIG.val_transforms\n",
    "                    }\n",
    "\n",
    "#Make No Call Dataframes\n",
    "df_nocall = pd.read_csv(os.path.join(CONFIG.birdclef2021_background_noise, 'ff1010bird_metadata_v1.csv'))\n",
    "df_train_nocall, df_valid_nocall = train_test_split(df_nocall, test_size=CONFIG.val_frac)\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_train_full = pd.concat([train_df, df_train_nocall], axis = 0)\n",
    "df_valid_full = pd.concat([val_df, df_valid_nocall], axis = 0)\n",
    "\n",
    "# Reset the index to create a new index for the concatenated DataFrame\n",
    "df_train_full.reset_index(drop=True, inplace=True)\n",
    "df_valid_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make Datasets\n",
    "train_dataset_nocall = NoCallDataset(datapath=[0,CONFIG.birdclef2021_background_noise], metadata_df=df_train_nocall, audio_transforms=CONFIG.train_transforms_audio, sample_rate=CONFIG.sample_rate, period=CONFIG.period, inherited_species_list=species_list)\n",
    "valid_dataset_nocall = NoCallDataset(datapath=[0,CONFIG.birdclef2021_background_noise], metadata_df=df_valid_nocall, audio_transforms=CONFIG.val_transforms_audio, sample_rate=CONFIG.sample_rate, period=CONFIG.period, inherited_species_list=species_list)\n",
    "\n",
    "# Make dataset\n",
    "if CONFIG.use_nocall == True:\n",
    "    train_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023, CONFIG.birdclef2021_background_noise], metadata_df=df_train_full, audio_transforms=CONFIG.train_transforms_audio ,sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list)\n",
    "    valid_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023, CONFIG.birdclef2021_background_noise], metadata_df=df_valid_full, audio_transforms=CONFIG.val_transforms_audio, sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list)\n",
    "else:\n",
    "    train_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023], metadata_df=train_df, audio_transforms=CONFIG.train_transforms_audio ,sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list, backgroundData=train_dataset_nocall)\n",
    "    valid_dataset = BirdCLEF2023(datapath=[CONFIG.birdclef2023], metadata_df=val_df, audio_transforms=CONFIG.val_transforms_audio, sample_rate=CONFIG.sample_rate, soft_second_label=CONFIG.soft_second_label, period=CONFIG.period, inherited_species_list=species_list, backgroundData=valid_dataset_nocall)\n",
    "\n",
    "if CONFIG.uniform_sampler == False:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.train_batch_size, shuffle = True, pin_memory = True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.valid_batch_size, shuffle = True, pin_memory = True)\n",
    "else:\n",
    "    train_sampler = make_uniformSampler(dataset=train_dataset)\n",
    "    valid_sampler = make_uniformSampler(dataset=valid_dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.train_batch_size, pin_memory = True, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,num_workers=CONFIG.num_workers, batch_size=CONFIG.valid_batch_size, pin_memory = True, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 238/239 [07:50<00:01,  1.98s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m species_count\u001b[39m.\u001b[39mnumpy(), total_samples\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m count_int_train, n_train \u001b[39m=\u001b[39m test_sampler(dataloader\u001b[39m=\u001b[39;49mtrain_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m count_int_val, n_val \u001b[39m=\u001b[39m test_sampler(dataloader\u001b[39m=\u001b[39mvalid_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m penguin_means \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mTraining Set\u001b[39m\u001b[39m'\u001b[39m: count_int_train \u001b[39m/\u001b[39m n_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mValidation Set\u001b[39m\u001b[39m'\u001b[39m: count_int_val \u001b[39m/\u001b[39m n_val\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m }\n",
      "\u001b[1;32m/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         batch_size \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         label_idx \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(labels\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         species_count[label_idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colin/elec5305/ele5305_research_project/src_audio/train_notebook.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m species_count\u001b[39m.\u001b[39mnumpy(), total_samples\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "def test_sampler(dataloader, epochs = 1):\n",
    "\n",
    "    species_list = dataloader.dataset.species\n",
    "    species_count = torch.tensor([0] * len(species_list))\n",
    "    total_samples = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(tqdm(dataloader, desc=\"Processing\")):\n",
    "            labels = data[1]\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "\n",
    "            label_idx = list(labels.argmax(1))\n",
    "            species_count[label_idx] += 1\n",
    "            total_samples += batch_size\n",
    "\n",
    "    return species_count.numpy(), total_samples\n",
    "\n",
    "\n",
    "# count_int_train, n_train = test_sampler(dataloader=train_loader)\n",
    "# count_int_val, n_val = test_sampler(dataloader=valid_loader)\n",
    "\n",
    "# penguin_means = {\n",
    "#     'Training Set': count_int_train / n_train,\n",
    "#     'Validation Set': count_int_val / n_val\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# x = np.arange(len(species_list))  # the label locations\n",
    "# width = 0.35  # the width of the bars\n",
    "# multiplier = 0\n",
    "\n",
    "# fig, ax = plt.subplots(layout='constrained', figsize=(24,12))\n",
    "# colors = ['crimson','midnightblue']\n",
    "# for i,(attribute, measurement) in enumerate(penguin_means.items()):\n",
    "#     offset = width * i\n",
    "#     rects = ax.bar(x + offset, measurement, width, label=attribute, color=colors[i], alpha=0.7)\n",
    "#     multiplier += 1\n",
    "\n",
    "# # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# ax.set_ylabel('Fraction of Dataset [-]')\n",
    "# ax.set_title('Distribution of Classes in Validation and Training Data')\n",
    "\n",
    "# # ax.set_xticks(x + width / 2, classes_lsit)\n",
    "# # ax.set_xticks(x + width / 2)\n",
    "\n",
    "# ax.legend(loc='upper left', ncols=2)\n",
    "# ax.set_ylim(0, 0.1)\n",
    "# plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric Function as on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_cmap(solution, submission, padding_factor=5):\n",
    "    solution = solution.drop(['row_id'], axis=1, errors='ignore')\n",
    "    submission = submission.drop(['row_id'], axis=1, errors='ignore')\n",
    "    new_rows = []\n",
    "    for i in range(padding_factor):\n",
    "        new_rows.append([1 for i in range(len(solution.columns))])\n",
    "    new_rows = pd.DataFrame(new_rows)\n",
    "    new_rows.columns = solution.columns\n",
    "    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n",
    "    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n",
    "    score = sklearn.metrics.average_precision_score(\n",
    "        padded_solution.values,\n",
    "        padded_submission.values,\n",
    "        average='macro',\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(final_output_path):\n",
    "    log_file = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    head = '%(asctime)-15s %(message)s'\n",
    "    logging.basicConfig(filename=os.path.join(final_output_path, log_file),\n",
    "                        format=head)\n",
    "    clogger = logging.getLogger()\n",
    "    clogger.setLevel(logging.INFO)\n",
    "    # add handler\n",
    "    # print to stdout and log file\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    clogger.addHandler(ch)\n",
    "    return clogger\n",
    "\n",
    "def train_with_mixup(X, y, y_pred, criterion):\n",
    "    X, y_a, y_b, lam = mixup_data(X, y, alpha=CONFIG.mixup_alpha)\n",
    "    loss_mixup = mixup_criterion(criterion, y_pred, y_a, y_b, lam) #cross_entropy\n",
    "    return loss_mixup\n",
    "\n",
    "def train_net(net, trainloader, valloader, logging, criterion, optimizer, scheduler, epochs=1, patience = 3, savePth = 'project2_weights.pth', print_every_samples = 20, device = 'cpu'):\n",
    "\n",
    "    logging.info('Using device: {}'.format(device))\n",
    "    net.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    if CONFIG.softmax_prob == True:\n",
    "        toProb = torch.nn.Softmax(dim=1)\n",
    "    else:\n",
    "        toProb = torch.nn.Identity()\n",
    "\n",
    "    # Automatic Mixed Precision\n",
    "    if CONFIG.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=CONFIG.use_amp)\n",
    "\n",
    "    validation_loss_list = [0] * epochs\n",
    "    training_loss_list = [0] * epochs\n",
    "    validation_accuracy_list = [0] * epochs\n",
    "    training_accuracy_list = [0] * epochs\n",
    "    cmap_5_list = [0] * epochs\n",
    "\n",
    "    best_state_dictionary = None\n",
    "    best_validation_cmap = 0.0\n",
    "    inertia = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        training_accuracy = 0.0\n",
    "        running_loss = 0.0\n",
    "        # Set model to training mode\n",
    "        net.mel_generator.transform = CONFIG.train_transforms\n",
    "        net = net.train()\n",
    "\n",
    "        # Calculate the number of batches to loop over\n",
    "        num_batches_to_loop = int(CONFIG.training_data_per_epoch * len(trainloader))\n",
    "        with tqdm(enumerate(trainloader, 0), total=num_batches_to_loop, desc=\"Training Batches Epoch {} / {}\".format(epoch + 1, epochs)) as train_pbar:\n",
    "            for i, data in train_pbar:\n",
    "        \n",
    "                # get the inputs\n",
    "                if device == 'cuda':\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                else:\n",
    "                    inputs, labels = data\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if CONFIG.use_amp:\n",
    "                    with torch.autocast(device_type=device, dtype=torch.float16, enabled=CONFIG.use_amp):\n",
    "\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = net(inputs)\n",
    "\n",
    "                        if CONFIG.use_mixup:\n",
    "                            loss_value = train_with_mixup(inputs, labels, outputs, criterion=criterion)\n",
    "                        else:\n",
    "                            loss_value = criterion(outputs,labels)\n",
    "\n",
    "                    # loss_value.backward()\n",
    "                    # optimizer.step()\n",
    "                    scaler.scale(loss_value).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # forward + backward + optimize\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    if CONFIG.use_mixup:\n",
    "                        loss_value = train_with_mixup(inputs, labels, outputs, criterion=criterion)\n",
    "                    else:\n",
    "                        loss_value = criterion(outputs,labels)\n",
    "\n",
    "                    loss_value.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print statistics and write to log\n",
    "                running_loss += loss_value.item()\n",
    "                training_loss += loss_value.item()\n",
    "\n",
    "                train_pbar.set_postfix(loss=(running_loss / ((i + 1) * trainloader.batch_size)))\n",
    "                training_accuracy += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "\n",
    "                if type(scheduler).__name__ != 'NoneType':\n",
    "                    scheduler.step(epoch + i / len(trainloader))\n",
    "\n",
    "                if i >= num_batches_to_loop:\n",
    "                    break\n",
    "\n",
    "        # if type(scheduler).__name__ != 'NoneType':\n",
    "        #     scheduler.step()\n",
    "\n",
    "        training_loss = training_loss / (len(trainloader.dataset) * CONFIG.training_data_per_epoch)\n",
    "        training_loss_list[epoch] = training_loss\n",
    "        training_accuracy = 100 * training_accuracy / (len(trainloader.dataset) * CONFIG.training_data_per_epoch)\n",
    "        training_accuracy_list[epoch] = training_accuracy\n",
    "\n",
    "        logging.info('Batch {:5d} / {:5d}: Training Loss = {:.3f}, Training Accuracy = {:.3f}'.format(epoch + 1, epochs, training_loss, training_accuracy))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        predictions_array = np.zeros((len(valloader.dataset), len(valloader.dataset.species)), dtype=float)\n",
    "        solutions_array = np.zeros((len(valloader.dataset), len(valloader.dataset.species)), dtype=float)\n",
    "        # Set model to validation mode\n",
    "        net.mel_generator.transform = CONFIG.val_transforms\n",
    "        net = net.eval()\n",
    "        with tqdm(enumerate(valloader, 0), total=len(valloader), desc=\"Validation Batches Epoch {} / {}\".format(epoch + 1, epochs)) as val_pbar:\n",
    "            for i, data in val_pbar:\n",
    "                # get the inputs\n",
    "                if device == 'cuda':\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                else:\n",
    "                    inputs, labels = data\n",
    "                    \n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss_value = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics and write to log\n",
    "                running_loss += loss_value.item()\n",
    "                val_loss += loss_value.item()\n",
    "\n",
    "                # Get model output and label to array\n",
    "                curr_predictions_array = toProb(outputs).detach().cpu().numpy()\n",
    "                predictions_array[i*valloader.batch_size:(i+1)*valloader.batch_size,:] = curr_predictions_array\n",
    "                hardlabels = labels.detach().cpu().numpy()\n",
    "                hardlabels[hardlabels < 0.99] = 0\n",
    "                curr_solutions_array = hardlabels\n",
    "                solutions_array[i*valloader.batch_size:(i+1)*valloader.batch_size,:] = curr_solutions_array\n",
    "\n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix(loss=(running_loss / ((i + 1) * trainloader.batch_size)))\n",
    "                correct += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "        \n",
    "        # Get cMAP\n",
    "        cmap_5 = padded_cmap(solution=pd.DataFrame(solutions_array), submission=pd.DataFrame(predictions_array), padding_factor=5)\n",
    "\n",
    "        # Get Metrics\n",
    "        val_loss = val_loss / len(valloader.dataset)\n",
    "        validation_loss_list[epoch] = val_loss\n",
    "        val_accuracy = 100 * correct / len(valloader.dataset)\n",
    "        validation_accuracy_list[epoch] = val_accuracy\n",
    "        cmap_5_list[epoch] = cmap_5\n",
    "\n",
    "        logging.info('Batch {:5d} / {:5d}: Validation Loss = {:.3f}, Validation Accuracy = {:.3f}, cmap score = {:.3f}'.format(epoch + 1, epochs, val_loss, val_accuracy, cmap_5))\n",
    "\n",
    "        save_weights = os.path.join(savePth,'model_weights.pth')\n",
    "        if cmap_5 > best_validation_cmap:\n",
    "            best_validation_cmap = cmap_5\n",
    "            best_state_dictionary = copy.deepcopy(net.state_dict())\n",
    "            # save network\n",
    "            torch.save(best_state_dictionary, save_weights)\n",
    "            inertia = 0\n",
    "            logging.info('Epoch {:5d} / {:5d} saved: New Best Epoch!'.format(epoch + 1, epochs))\n",
    "        else:\n",
    "            inertia += 1\n",
    "            if inertia == patience:\n",
    "                if best_state_dictionary is None:\n",
    "                    raise Exception(\"State dictionary should have been updated at least once\")\n",
    "                break\n",
    "        # print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    logging.info('Finished Training')\n",
    "\n",
    "    output = {'validation_loss': validation_loss_list,\n",
    "              'validation_accuracy': validation_accuracy_list,\n",
    "              'training_loss': training_loss_list,\n",
    "              'training_accuracy': training_accuracy_list,\n",
    "              'cmap_5_scores': cmap_5_list}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train():\n",
    "\n",
    "    # Change Output path\n",
    "    folder_name = time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    outpath = os.path.join(CONFIG.outpath, folder_name)\n",
    "    CONFIG.outpath = outpath\n",
    "    # Create Output directory\n",
    "    os.makedirs(CONFIG.outpath, exist_ok=True)\n",
    "\n",
    "    # Create Logger\n",
    "    logger = create_logger(final_output_path=CONFIG.outpath)\n",
    "\n",
    "    # Get all variable to logger\n",
    "    logger.info('############################################ START CONFIG FILE ############################################')\n",
    "    for attr, value in vars(CONFIG).items():\n",
    "        logger.info(f\"{attr}: {value}\")\n",
    "    logger.info('############################################  END CONFIG FILE  ############################################')\n",
    "    config_dict = {attr: value for attr, value in vars(CONFIG).items()}\n",
    "    outputName = 'hyperparameters.json'\n",
    "    jsonpath = os.path.join(CONFIG.outpath, outputName)\n",
    "    with open(jsonpath, 'w') as json_file:\n",
    "        json.dump(str(config_dict), json_file)\n",
    "\n",
    "    melspec_layer = MelSpectrogramLayer(sample_rate=CONFIG.sample_rate,\n",
    "                                    n_fft=CONFIG.n_fft,\n",
    "                                    hop_length=CONFIG.hop_length,\n",
    "                                    n_mels=CONFIG.n_mels,\n",
    "                                    transform=CONFIG.train_transforms)\n",
    "    network = Mel_Classifier(model_name=CONFIG.model_name,\n",
    "                            mel_generator=melspec_layer)\n",
    "    \n",
    "    if CONFIG.load_pretrained_weights == True:\n",
    "        logger.info('Load PreTrained Weigths')\n",
    "        network.load_state_dict(torch.load(CONFIG.pretrained_weights, map_location=CONFIG.device))\n",
    "\n",
    "    if CONFIG.fix_features == True:\n",
    "        for param in network.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if CONFIG.class_weighting == True:\n",
    "        class_weights = torch.tensor(make_class_weights(train_dataset))\n",
    "        criterion = CONFIG.criterion(weight=class_weights)\n",
    "    else:\n",
    "        criterion = CONFIG.criterion()\n",
    "        \n",
    "    if CONFIG.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, network.parameters()), lr=CONFIG.lr, weight_decay=CONFIG.weight_decay)\n",
    "    elif CONFIG.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, network.parameters()), lr=CONFIG.lr, momentum=CONFIG.momentum)\n",
    "    if CONFIG.scheduler == 'cosineannealing':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                            optimizer, \n",
    "                            T_0=CONFIG.epochs, \n",
    "                            T_mult=CONFIG.T_mult, \n",
    "                            eta_min=CONFIG.eta_min, \n",
    "                            last_epoch=CONFIG.last_epoch\n",
    "                        )\n",
    "    elif CONFIG.scheduler == None:\n",
    "        CONFIG.scheduler = None\n",
    "\n",
    "    # Train Net\n",
    "    output = train_net( net=network,\n",
    "                        trainloader=train_loader,\n",
    "                        valloader=valid_loader,\n",
    "                        criterion=criterion,\n",
    "                        optimizer=optimizer,\n",
    "                        logging=logger,\n",
    "                        scheduler=scheduler,\n",
    "                        epochs=CONFIG.epochs,\n",
    "                        device=CONFIG.device,\n",
    "                        print_every_samples=CONFIG.print_every_n_batches,\n",
    "                        savePth=CONFIG.outpath,\n",
    "                        patience=CONFIG.patience\n",
    "                        )\n",
    "    \n",
    "    # Save Output\n",
    "    outputName = 'training_prog.json'\n",
    "    jsonpath = os.path.join(CONFIG.outpath, outputName)\n",
    "    with open(jsonpath, 'w') as json_file:\n",
    "        json.dump(output, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:24:32,410 - root - INFO - ############################################ START CONFIG FILE ############################################\n",
      "2023-11-08 15:24:32,410 - root - INFO - ############################################ START CONFIG FILE ############################################\n",
      "2023-11-08 15:24:32,410 - root - INFO - ############################################ START CONFIG FILE ############################################\n",
      "2023-11-08 15:24:32,410 - root - INFO - ############################################ START CONFIG FILE ############################################\n",
      "2023-11-08 15:24:32,410 - root - INFO - ############################################ START CONFIG FILE ############################################\n",
      "2023-11-08 15:24:32,413 - root - INFO - run_environment: local\n",
      "2023-11-08 15:24:32,413 - root - INFO - run_environment: local\n",
      "2023-11-08 15:24:32,413 - root - INFO - run_environment: local\n",
      "2023-11-08 15:24:32,413 - root - INFO - run_environment: local\n",
      "2023-11-08 15:24:32,413 - root - INFO - run_environment: local\n",
      "2023-11-08 15:24:32,418 - root - INFO - load_pretrained_weights: False\n",
      "2023-11-08 15:24:32,418 - root - INFO - load_pretrained_weights: False\n",
      "2023-11-08 15:24:32,418 - root - INFO - load_pretrained_weights: False\n",
      "2023-11-08 15:24:32,418 - root - INFO - load_pretrained_weights: False\n",
      "2023-11-08 15:24:32,418 - root - INFO - load_pretrained_weights: False\n",
      "2023-11-08 15:24:32,424 - root - INFO - rerun_split: False\n",
      "2023-11-08 15:24:32,424 - root - INFO - rerun_split: False\n",
      "2023-11-08 15:24:32,424 - root - INFO - rerun_split: False\n",
      "2023-11-08 15:24:32,424 - root - INFO - rerun_split: False\n",
      "2023-11-08 15:24:32,424 - root - INFO - rerun_split: False\n",
      "2023-11-08 15:24:32,429 - root - INFO - use_5_second_dataset: True\n",
      "2023-11-08 15:24:32,429 - root - INFO - use_5_second_dataset: True\n",
      "2023-11-08 15:24:32,429 - root - INFO - use_5_second_dataset: True\n",
      "2023-11-08 15:24:32,429 - root - INFO - use_5_second_dataset: True\n",
      "2023-11-08 15:24:32,429 - root - INFO - use_5_second_dataset: True\n",
      "2023-11-08 15:24:32,433 - root - INFO - uniform_sampler: True\n",
      "2023-11-08 15:24:32,433 - root - INFO - uniform_sampler: True\n",
      "2023-11-08 15:24:32,433 - root - INFO - uniform_sampler: True\n",
      "2023-11-08 15:24:32,433 - root - INFO - uniform_sampler: True\n",
      "2023-11-08 15:24:32,433 - root - INFO - uniform_sampler: True\n",
      "2023-11-08 15:24:32,435 - root - INFO - training_data_per_epoch: 0.25\n",
      "2023-11-08 15:24:32,435 - root - INFO - training_data_per_epoch: 0.25\n",
      "2023-11-08 15:24:32,435 - root - INFO - training_data_per_epoch: 0.25\n",
      "2023-11-08 15:24:32,435 - root - INFO - training_data_per_epoch: 0.25\n",
      "2023-11-08 15:24:32,435 - root - INFO - training_data_per_epoch: 0.25\n",
      "2023-11-08 15:24:32,441 - root - INFO - soft_second_label: 0.3\n",
      "2023-11-08 15:24:32,441 - root - INFO - soft_second_label: 0.3\n",
      "2023-11-08 15:24:32,441 - root - INFO - soft_second_label: 0.3\n",
      "2023-11-08 15:24:32,441 - root - INFO - soft_second_label: 0.3\n",
      "2023-11-08 15:24:32,441 - root - INFO - soft_second_label: 0.3\n",
      "2023-11-08 15:24:32,445 - root - INFO - class_weighting: True\n",
      "2023-11-08 15:24:32,445 - root - INFO - class_weighting: True\n",
      "2023-11-08 15:24:32,445 - root - INFO - class_weighting: True\n",
      "2023-11-08 15:24:32,445 - root - INFO - class_weighting: True\n",
      "2023-11-08 15:24:32,445 - root - INFO - class_weighting: True\n",
      "2023-11-08 15:24:32,449 - root - INFO - softmax_prob: True\n",
      "2023-11-08 15:24:32,449 - root - INFO - softmax_prob: True\n",
      "2023-11-08 15:24:32,449 - root - INFO - softmax_prob: True\n",
      "2023-11-08 15:24:32,449 - root - INFO - softmax_prob: True\n",
      "2023-11-08 15:24:32,449 - root - INFO - softmax_prob: True\n",
      "2023-11-08 15:24:32,452 - root - INFO - use_mixup: True\n",
      "2023-11-08 15:24:32,452 - root - INFO - use_mixup: True\n",
      "2023-11-08 15:24:32,452 - root - INFO - use_mixup: True\n",
      "2023-11-08 15:24:32,452 - root - INFO - use_mixup: True\n",
      "2023-11-08 15:24:32,452 - root - INFO - use_mixup: True\n",
      "2023-11-08 15:24:32,454 - root - INFO - criterion: <class '__main__.LabelSmoothingBCEWithLogitsLoss'>\n",
      "2023-11-08 15:24:32,454 - root - INFO - criterion: <class '__main__.LabelSmoothingBCEWithLogitsLoss'>\n",
      "2023-11-08 15:24:32,454 - root - INFO - criterion: <class '__main__.LabelSmoothingBCEWithLogitsLoss'>\n",
      "2023-11-08 15:24:32,454 - root - INFO - criterion: <class '__main__.LabelSmoothingBCEWithLogitsLoss'>\n",
      "2023-11-08 15:24:32,454 - root - INFO - criterion: <class '__main__.LabelSmoothingBCEWithLogitsLoss'>\n",
      "2023-11-08 15:24:32,460 - root - INFO - epochs: 25\n",
      "2023-11-08 15:24:32,460 - root - INFO - epochs: 25\n",
      "2023-11-08 15:24:32,460 - root - INFO - epochs: 25\n",
      "2023-11-08 15:24:32,460 - root - INFO - epochs: 25\n",
      "2023-11-08 15:24:32,460 - root - INFO - epochs: 25\n",
      "2023-11-08 15:24:32,463 - root - INFO - masking: True\n",
      "2023-11-08 15:24:32,463 - root - INFO - masking: True\n",
      "2023-11-08 15:24:32,463 - root - INFO - masking: True\n",
      "2023-11-08 15:24:32,463 - root - INFO - masking: True\n",
      "2023-11-08 15:24:32,463 - root - INFO - masking: True\n",
      "2023-11-08 15:24:32,467 - root - INFO - masking_prob: 0.5\n",
      "2023-11-08 15:24:32,467 - root - INFO - masking_prob: 0.5\n",
      "2023-11-08 15:24:32,467 - root - INFO - masking_prob: 0.5\n",
      "2023-11-08 15:24:32,467 - root - INFO - masking_prob: 0.5\n",
      "2023-11-08 15:24:32,467 - root - INFO - masking_prob: 0.5\n",
      "2023-11-08 15:24:32,470 - root - INFO - frac_nocall: 0.3\n",
      "2023-11-08 15:24:32,470 - root - INFO - frac_nocall: 0.3\n",
      "2023-11-08 15:24:32,470 - root - INFO - frac_nocall: 0.3\n",
      "2023-11-08 15:24:32,470 - root - INFO - frac_nocall: 0.3\n",
      "2023-11-08 15:24:32,470 - root - INFO - frac_nocall: 0.3\n",
      "2023-11-08 15:24:32,475 - root - INFO - use_nocall: False\n",
      "2023-11-08 15:24:32,475 - root - INFO - use_nocall: False\n",
      "2023-11-08 15:24:32,475 - root - INFO - use_nocall: False\n",
      "2023-11-08 15:24:32,475 - root - INFO - use_nocall: False\n",
      "2023-11-08 15:24:32,475 - root - INFO - use_nocall: False\n",
      "2023-11-08 15:24:32,478 - root - INFO - addbackground_prob: 0.5\n",
      "2023-11-08 15:24:32,478 - root - INFO - addbackground_prob: 0.5\n",
      "2023-11-08 15:24:32,478 - root - INFO - addbackground_prob: 0.5\n",
      "2023-11-08 15:24:32,478 - root - INFO - addbackground_prob: 0.5\n",
      "2023-11-08 15:24:32,478 - root - INFO - addbackground_prob: 0.5\n",
      "2023-11-08 15:24:32,481 - root - INFO - device: cpu\n",
      "2023-11-08 15:24:32,481 - root - INFO - device: cpu\n",
      "2023-11-08 15:24:32,481 - root - INFO - device: cpu\n",
      "2023-11-08 15:24:32,481 - root - INFO - device: cpu\n",
      "2023-11-08 15:24:32,481 - root - INFO - device: cpu\n",
      "2023-11-08 15:24:32,487 - root - INFO - birdclef2023_melspectrograms: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms\n",
      "2023-11-08 15:24:32,487 - root - INFO - birdclef2023_melspectrograms: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms\n",
      "2023-11-08 15:24:32,487 - root - INFO - birdclef2023_melspectrograms: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms\n",
      "2023-11-08 15:24:32,487 - root - INFO - birdclef2023_melspectrograms: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms\n",
      "2023-11-08 15:24:32,487 - root - INFO - birdclef2023_melspectrograms: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms\n",
      "2023-11-08 15:24:32,491 - root - INFO - birdclef2023_melspectrograms_5_seconds: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms_5_seconds\n",
      "2023-11-08 15:24:32,491 - root - INFO - birdclef2023_melspectrograms_5_seconds: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms_5_seconds\n",
      "2023-11-08 15:24:32,491 - root - INFO - birdclef2023_melspectrograms_5_seconds: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms_5_seconds\n",
      "2023-11-08 15:24:32,491 - root - INFO - birdclef2023_melspectrograms_5_seconds: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms_5_seconds\n",
      "2023-11-08 15:24:32,491 - root - INFO - birdclef2023_melspectrograms_5_seconds: /home/colin/elec5305/ele5305_research_project/birdclef-2023_MelSpectrograms_5_seconds\n",
      "2023-11-08 15:24:32,495 - root - INFO - birdclef2023: /home/colin/elec5305/ele5305_research_project/birdclef-2023\n",
      "2023-11-08 15:24:32,495 - root - INFO - birdclef2023: /home/colin/elec5305/ele5305_research_project/birdclef-2023\n",
      "2023-11-08 15:24:32,495 - root - INFO - birdclef2023: /home/colin/elec5305/ele5305_research_project/birdclef-2023\n",
      "2023-11-08 15:24:32,495 - root - INFO - birdclef2023: /home/colin/elec5305/ele5305_research_project/birdclef-2023\n",
      "2023-11-08 15:24:32,495 - root - INFO - birdclef2023: /home/colin/elec5305/ele5305_research_project/birdclef-2023\n",
      "2023-11-08 15:24:32,498 - root - INFO - birdclef2021_background_noise: /home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall\n",
      "2023-11-08 15:24:32,498 - root - INFO - birdclef2021_background_noise: /home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall\n",
      "2023-11-08 15:24:32,498 - root - INFO - birdclef2021_background_noise: /home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall\n",
      "2023-11-08 15:24:32,498 - root - INFO - birdclef2021_background_noise: /home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall\n",
      "2023-11-08 15:24:32,498 - root - INFO - birdclef2021_background_noise: /home/colin/elec5305/ele5305_research_project/birdclef2021_background_noise/ff1010bird_nocall\n",
      "2023-11-08 15:24:32,502 - root - INFO - outpath: results/2023-11-08_15-24-32\n",
      "2023-11-08 15:24:32,502 - root - INFO - outpath: results/2023-11-08_15-24-32\n",
      "2023-11-08 15:24:32,502 - root - INFO - outpath: results/2023-11-08_15-24-32\n",
      "2023-11-08 15:24:32,502 - root - INFO - outpath: results/2023-11-08_15-24-32\n",
      "2023-11-08 15:24:32,502 - root - INFO - outpath: results/2023-11-08_15-24-32\n",
      "2023-11-08 15:24:32,507 - root - INFO - val_frac: 0.1\n",
      "2023-11-08 15:24:32,507 - root - INFO - val_frac: 0.1\n",
      "2023-11-08 15:24:32,507 - root - INFO - val_frac: 0.1\n",
      "2023-11-08 15:24:32,507 - root - INFO - val_frac: 0.1\n",
      "2023-11-08 15:24:32,507 - root - INFO - val_frac: 0.1\n",
      "2023-11-08 15:24:32,510 - root - INFO - num_workers: 2\n",
      "2023-11-08 15:24:32,510 - root - INFO - num_workers: 2\n",
      "2023-11-08 15:24:32,510 - root - INFO - num_workers: 2\n",
      "2023-11-08 15:24:32,510 - root - INFO - num_workers: 2\n",
      "2023-11-08 15:24:32,510 - root - INFO - num_workers: 2\n",
      "2023-11-08 15:24:32,515 - root - INFO - train_batch_size: 64\n",
      "2023-11-08 15:24:32,515 - root - INFO - train_batch_size: 64\n",
      "2023-11-08 15:24:32,515 - root - INFO - train_batch_size: 64\n",
      "2023-11-08 15:24:32,515 - root - INFO - train_batch_size: 64\n",
      "2023-11-08 15:24:32,515 - root - INFO - train_batch_size: 64\n",
      "2023-11-08 15:24:32,523 - root - INFO - valid_batch_size: 32\n",
      "2023-11-08 15:24:32,523 - root - INFO - valid_batch_size: 32\n",
      "2023-11-08 15:24:32,523 - root - INFO - valid_batch_size: 32\n",
      "2023-11-08 15:24:32,523 - root - INFO - valid_batch_size: 32\n",
      "2023-11-08 15:24:32,523 - root - INFO - valid_batch_size: 32\n",
      "2023-11-08 15:24:32,534 - root - INFO - model_name: tf_efficientnet_b0_ns\n",
      "2023-11-08 15:24:32,534 - root - INFO - model_name: tf_efficientnet_b0_ns\n",
      "2023-11-08 15:24:32,534 - root - INFO - model_name: tf_efficientnet_b0_ns\n",
      "2023-11-08 15:24:32,534 - root - INFO - model_name: tf_efficientnet_b0_ns\n",
      "2023-11-08 15:24:32,534 - root - INFO - model_name: tf_efficientnet_b0_ns\n",
      "2023-11-08 15:24:32,538 - root - INFO - pretrained_weights: /home/colin/elec5305/ele5305_research_project/weights/model_weights.pth\n",
      "2023-11-08 15:24:32,538 - root - INFO - pretrained_weights: /home/colin/elec5305/ele5305_research_project/weights/model_weights.pth\n",
      "2023-11-08 15:24:32,538 - root - INFO - pretrained_weights: /home/colin/elec5305/ele5305_research_project/weights/model_weights.pth\n",
      "2023-11-08 15:24:32,538 - root - INFO - pretrained_weights: /home/colin/elec5305/ele5305_research_project/weights/model_weights.pth\n",
      "2023-11-08 15:24:32,538 - root - INFO - pretrained_weights: /home/colin/elec5305/ele5305_research_project/weights/model_weights.pth\n",
      "2023-11-08 15:24:32,541 - root - INFO - lr: 0.0005\n",
      "2023-11-08 15:24:32,541 - root - INFO - lr: 0.0005\n",
      "2023-11-08 15:24:32,541 - root - INFO - lr: 0.0005\n",
      "2023-11-08 15:24:32,541 - root - INFO - lr: 0.0005\n",
      "2023-11-08 15:24:32,541 - root - INFO - lr: 0.0005\n",
      "2023-11-08 15:24:32,546 - root - INFO - weight_decay: 0.001\n",
      "2023-11-08 15:24:32,546 - root - INFO - weight_decay: 0.001\n",
      "2023-11-08 15:24:32,546 - root - INFO - weight_decay: 0.001\n",
      "2023-11-08 15:24:32,546 - root - INFO - weight_decay: 0.001\n",
      "2023-11-08 15:24:32,546 - root - INFO - weight_decay: 0.001\n",
      "2023-11-08 15:24:32,549 - root - INFO - momentum: 0.9\n",
      "2023-11-08 15:24:32,549 - root - INFO - momentum: 0.9\n",
      "2023-11-08 15:24:32,549 - root - INFO - momentum: 0.9\n",
      "2023-11-08 15:24:32,549 - root - INFO - momentum: 0.9\n",
      "2023-11-08 15:24:32,549 - root - INFO - momentum: 0.9\n",
      "2023-11-08 15:24:32,552 - root - INFO - optimizer: adam\n",
      "2023-11-08 15:24:32,552 - root - INFO - optimizer: adam\n",
      "2023-11-08 15:24:32,552 - root - INFO - optimizer: adam\n",
      "2023-11-08 15:24:32,552 - root - INFO - optimizer: adam\n",
      "2023-11-08 15:24:32,552 - root - INFO - optimizer: adam\n",
      "2023-11-08 15:24:32,556 - root - INFO - scheduler: cosineannealing\n",
      "2023-11-08 15:24:32,556 - root - INFO - scheduler: cosineannealing\n",
      "2023-11-08 15:24:32,556 - root - INFO - scheduler: cosineannealing\n",
      "2023-11-08 15:24:32,556 - root - INFO - scheduler: cosineannealing\n",
      "2023-11-08 15:24:32,556 - root - INFO - scheduler: cosineannealing\n",
      "2023-11-08 15:24:32,559 - root - INFO - eta_min: 1e-06\n",
      "2023-11-08 15:24:32,559 - root - INFO - eta_min: 1e-06\n",
      "2023-11-08 15:24:32,559 - root - INFO - eta_min: 1e-06\n",
      "2023-11-08 15:24:32,559 - root - INFO - eta_min: 1e-06\n",
      "2023-11-08 15:24:32,559 - root - INFO - eta_min: 1e-06\n",
      "2023-11-08 15:24:32,562 - root - INFO - T_mult: 1\n",
      "2023-11-08 15:24:32,562 - root - INFO - T_mult: 1\n",
      "2023-11-08 15:24:32,562 - root - INFO - T_mult: 1\n",
      "2023-11-08 15:24:32,562 - root - INFO - T_mult: 1\n",
      "2023-11-08 15:24:32,562 - root - INFO - T_mult: 1\n",
      "2023-11-08 15:24:32,566 - root - INFO - last_epoch: -1\n",
      "2023-11-08 15:24:32,566 - root - INFO - last_epoch: -1\n",
      "2023-11-08 15:24:32,566 - root - INFO - last_epoch: -1\n",
      "2023-11-08 15:24:32,566 - root - INFO - last_epoch: -1\n",
      "2023-11-08 15:24:32,566 - root - INFO - last_epoch: -1\n",
      "2023-11-08 15:24:32,569 - root - INFO - use_amp: False\n",
      "2023-11-08 15:24:32,569 - root - INFO - use_amp: False\n",
      "2023-11-08 15:24:32,569 - root - INFO - use_amp: False\n",
      "2023-11-08 15:24:32,569 - root - INFO - use_amp: False\n",
      "2023-11-08 15:24:32,569 - root - INFO - use_amp: False\n",
      "2023-11-08 15:24:32,571 - root - INFO - mixup_alpha: 0.5\n",
      "2023-11-08 15:24:32,571 - root - INFO - mixup_alpha: 0.5\n",
      "2023-11-08 15:24:32,571 - root - INFO - mixup_alpha: 0.5\n",
      "2023-11-08 15:24:32,571 - root - INFO - mixup_alpha: 0.5\n",
      "2023-11-08 15:24:32,571 - root - INFO - mixup_alpha: 0.5\n",
      "2023-11-08 15:24:32,574 - root - INFO - print_every_n_batches: 25\n",
      "2023-11-08 15:24:32,574 - root - INFO - print_every_n_batches: 25\n",
      "2023-11-08 15:24:32,574 - root - INFO - print_every_n_batches: 25\n",
      "2023-11-08 15:24:32,574 - root - INFO - print_every_n_batches: 25\n",
      "2023-11-08 15:24:32,574 - root - INFO - print_every_n_batches: 25\n",
      "2023-11-08 15:24:32,576 - root - INFO - patience: 5\n",
      "2023-11-08 15:24:32,576 - root - INFO - patience: 5\n",
      "2023-11-08 15:24:32,576 - root - INFO - patience: 5\n",
      "2023-11-08 15:24:32,576 - root - INFO - patience: 5\n",
      "2023-11-08 15:24:32,576 - root - INFO - patience: 5\n",
      "2023-11-08 15:24:32,579 - root - INFO - fix_features: False\n",
      "2023-11-08 15:24:32,579 - root - INFO - fix_features: False\n",
      "2023-11-08 15:24:32,579 - root - INFO - fix_features: False\n",
      "2023-11-08 15:24:32,579 - root - INFO - fix_features: False\n",
      "2023-11-08 15:24:32,579 - root - INFO - fix_features: False\n",
      "2023-11-08 15:24:32,587 - root - INFO - train_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "    RandomResizedCrop(size=(128, 312), scale=(0.75, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      ")\n",
      "2023-11-08 15:24:32,587 - root - INFO - train_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "    RandomResizedCrop(size=(128, 312), scale=(0.75, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      ")\n",
      "2023-11-08 15:24:32,587 - root - INFO - train_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "    RandomResizedCrop(size=(128, 312), scale=(0.75, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      ")\n",
      "2023-11-08 15:24:32,587 - root - INFO - train_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "    RandomResizedCrop(size=(128, 312), scale=(0.75, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      ")\n",
      "2023-11-08 15:24:32,587 - root - INFO - train_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "    RandomResizedCrop(size=(128, 312), scale=(0.75, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      ")\n",
      "2023-11-08 15:24:32,589 - root - INFO - val_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "2023-11-08 15:24:32,589 - root - INFO - val_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "2023-11-08 15:24:32,589 - root - INFO - val_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "2023-11-08 15:24:32,589 - root - INFO - val_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "2023-11-08 15:24:32,589 - root - INFO - val_transforms: Compose(\n",
      "    AmplitudeToDB()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "2023-11-08 15:24:32,592 - root - INFO - test_transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "2023-11-08 15:24:32,592 - root - INFO - test_transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "2023-11-08 15:24:32,592 - root - INFO - test_transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "2023-11-08 15:24:32,592 - root - INFO - test_transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "2023-11-08 15:24:32,592 - root - INFO - test_transforms: Compose(\n",
      "    ToTensor()\n",
      ")\n",
      "2023-11-08 15:24:32,596 - root - INFO - train_transforms_audio: None\n",
      "2023-11-08 15:24:32,596 - root - INFO - train_transforms_audio: None\n",
      "2023-11-08 15:24:32,596 - root - INFO - train_transforms_audio: None\n",
      "2023-11-08 15:24:32,596 - root - INFO - train_transforms_audio: None\n",
      "2023-11-08 15:24:32,596 - root - INFO - train_transforms_audio: None\n",
      "2023-11-08 15:24:32,599 - root - INFO - val_transforms_audio: None\n",
      "2023-11-08 15:24:32,599 - root - INFO - val_transforms_audio: None\n",
      "2023-11-08 15:24:32,599 - root - INFO - val_transforms_audio: None\n",
      "2023-11-08 15:24:32,599 - root - INFO - val_transforms_audio: None\n",
      "2023-11-08 15:24:32,599 - root - INFO - val_transforms_audio: None\n",
      "2023-11-08 15:24:32,605 - root - INFO - test_transforms_audio: None\n",
      "2023-11-08 15:24:32,605 - root - INFO - test_transforms_audio: None\n",
      "2023-11-08 15:24:32,605 - root - INFO - test_transforms_audio: None\n",
      "2023-11-08 15:24:32,605 - root - INFO - test_transforms_audio: None\n",
      "2023-11-08 15:24:32,605 - root - INFO - test_transforms_audio: None\n",
      "2023-11-08 15:24:32,608 - root - INFO - sample_rate: 32000\n",
      "2023-11-08 15:24:32,608 - root - INFO - sample_rate: 32000\n",
      "2023-11-08 15:24:32,608 - root - INFO - sample_rate: 32000\n",
      "2023-11-08 15:24:32,608 - root - INFO - sample_rate: 32000\n",
      "2023-11-08 15:24:32,608 - root - INFO - sample_rate: 32000\n",
      "2023-11-08 15:24:32,610 - root - INFO - period: 5\n",
      "2023-11-08 15:24:32,610 - root - INFO - period: 5\n",
      "2023-11-08 15:24:32,610 - root - INFO - period: 5\n",
      "2023-11-08 15:24:32,610 - root - INFO - period: 5\n",
      "2023-11-08 15:24:32,610 - root - INFO - period: 5\n",
      "2023-11-08 15:24:32,613 - root - INFO - n_fft: 2048\n",
      "2023-11-08 15:24:32,613 - root - INFO - n_fft: 2048\n",
      "2023-11-08 15:24:32,613 - root - INFO - n_fft: 2048\n",
      "2023-11-08 15:24:32,613 - root - INFO - n_fft: 2048\n",
      "2023-11-08 15:24:32,613 - root - INFO - n_fft: 2048\n",
      "2023-11-08 15:24:32,617 - root - INFO - f_min: 40\n",
      "2023-11-08 15:24:32,617 - root - INFO - f_min: 40\n",
      "2023-11-08 15:24:32,617 - root - INFO - f_min: 40\n",
      "2023-11-08 15:24:32,617 - root - INFO - f_min: 40\n",
      "2023-11-08 15:24:32,617 - root - INFO - f_min: 40\n",
      "2023-11-08 15:24:32,622 - root - INFO - f_max: 15000\n",
      "2023-11-08 15:24:32,622 - root - INFO - f_max: 15000\n",
      "2023-11-08 15:24:32,622 - root - INFO - f_max: 15000\n",
      "2023-11-08 15:24:32,622 - root - INFO - f_max: 15000\n",
      "2023-11-08 15:24:32,622 - root - INFO - f_max: 15000\n",
      "2023-11-08 15:24:32,626 - root - INFO - hop_length: 512\n",
      "2023-11-08 15:24:32,626 - root - INFO - hop_length: 512\n",
      "2023-11-08 15:24:32,626 - root - INFO - hop_length: 512\n",
      "2023-11-08 15:24:32,626 - root - INFO - hop_length: 512\n",
      "2023-11-08 15:24:32,626 - root - INFO - hop_length: 512\n",
      "2023-11-08 15:24:32,629 - root - INFO - n_mels: 128\n",
      "2023-11-08 15:24:32,629 - root - INFO - n_mels: 128\n",
      "2023-11-08 15:24:32,629 - root - INFO - n_mels: 128\n",
      "2023-11-08 15:24:32,629 - root - INFO - n_mels: 128\n",
      "2023-11-08 15:24:32,629 - root - INFO - n_mels: 128\n",
      "2023-11-08 15:24:32,633 - root - INFO - mel_args: {'n_fft': 2048, 'f_min': 40, 'f_max': 15000, 'hop_length': 512, 'n_mels': 128}\n",
      "2023-11-08 15:24:32,633 - root - INFO - mel_args: {'n_fft': 2048, 'f_min': 40, 'f_max': 15000, 'hop_length': 512, 'n_mels': 128}\n",
      "2023-11-08 15:24:32,633 - root - INFO - mel_args: {'n_fft': 2048, 'f_min': 40, 'f_max': 15000, 'hop_length': 512, 'n_mels': 128}\n",
      "2023-11-08 15:24:32,633 - root - INFO - mel_args: {'n_fft': 2048, 'f_min': 40, 'f_max': 15000, 'hop_length': 512, 'n_mels': 128}\n",
      "2023-11-08 15:24:32,633 - root - INFO - mel_args: {'n_fft': 2048, 'f_min': 40, 'f_max': 15000, 'hop_length': 512, 'n_mels': 128}\n",
      "2023-11-08 15:24:32,637 - root - INFO - ############################################  END CONFIG FILE  ############################################\n",
      "2023-11-08 15:24:32,637 - root - INFO - ############################################  END CONFIG FILE  ############################################\n",
      "2023-11-08 15:24:32,637 - root - INFO - ############################################  END CONFIG FILE  ############################################\n",
      "2023-11-08 15:24:32,637 - root - INFO - ############################################  END CONFIG FILE  ############################################\n",
      "2023-11-08 15:24:32,637 - root - INFO - ############################################  END CONFIG FILE  ############################################\n",
      "2023-11-08 15:24:32,789 - timm.models._builder - INFO - Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b0.ns_jft_in1k)\n",
      "2023-11-08 15:24:32,789 - timm.models._builder - INFO - Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b0.ns_jft_in1k)\n",
      "2023-11-08 15:24:32,789 - timm.models._builder - INFO - Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b0.ns_jft_in1k)\n",
      "2023-11-08 15:24:32,789 - timm.models._builder - INFO - Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b0.ns_jft_in1k)\n",
      "2023-11-08 15:24:32,789 - timm.models._builder - INFO - Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b0.ns_jft_in1k)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/elec5305/ele5305_research_project/.venv/lib/python3.11/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:24:36,120 - timm.models._hub - INFO - [timm/tf_efficientnet_b0.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2023-11-08 15:24:36,120 - timm.models._hub - INFO - [timm/tf_efficientnet_b0.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2023-11-08 15:24:36,120 - timm.models._hub - INFO - [timm/tf_efficientnet_b0.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2023-11-08 15:24:36,120 - timm.models._hub - INFO - [timm/tf_efficientnet_b0.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2023-11-08 15:24:36,120 - timm.models._hub - INFO - [timm/tf_efficientnet_b0.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2023-11-08 15:24:38,507 - root - INFO - Using device: cpu\n",
      "2023-11-08 15:24:38,507 - root - INFO - Using device: cpu\n",
      "2023-11-08 15:24:38,507 - root - INFO - Using device: cpu\n",
      "2023-11-08 15:24:38,507 - root - INFO - Using device: cpu\n",
      "2023-11-08 15:24:38,507 - root - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches Epoch 1 / 25:   0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle: Zip Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# https://www.kaggle.com/code/hari31416/downloading-file-and-directory-from-kaggle\n",
    "def zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n",
    "    \"\"\"\n",
    "    zip all the files in a directory\n",
    "    \n",
    "    Parameters\n",
    "    _____\n",
    "    directory: str\n",
    "        directory needs to be zipped, defualt is current working directory\n",
    "        \n",
    "    file_name: str\n",
    "        the name of the zipped file (including .zip), default is 'directory.zip'\n",
    "        \n",
    "    Returns\n",
    "    _____\n",
    "    Creates a hyperlink, which can be used to download the zip file)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)\n",
    "    zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file_name in file:\n",
    "                pass\n",
    "            else:\n",
    "                zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "    return FileLink(file_name)\n",
    "\n",
    "if CONFIG.run_environment == 'kaggle':\n",
    "    zip_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix issue that not all labels are in validation set\n",
    "# TODO: Fix progress bar\n",
    "# TODO: adam?\n",
    "# TODO: cMAP\n",
    "# TODO: Train with mixup    \n",
    "# TODO: Dataset metrics\n",
    "# TODO: Uniform sampler accross classes\n",
    "# TODO: class weights\n",
    "# TODO: use only sub-dataset per epoch?\n",
    "# TODO: consider second label\n",
    "# TODO: Inference Script\n",
    "\n",
    "# TODO: add no-call samples\n",
    "# TODO: augmentations\n",
    "# TODO: include rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try class weights by 1st rank:\n",
    "sample_weights = (\n",
    "    all_primary_labels.value_counts() / \n",
    "    all_primary_labels.value_counts().sum()\n",
    ")  ** (-0.5)\n",
    "\n",
    "Also by 1st rank:\n",
    "\n",
    "Small inference tricks\n",
    "\n",
    "    Using temperature mean: pred = (pred**2).mean(axis=0) ** 0.5\n",
    "    Using Attention SED probs * 0.75 + Max Timewise probs * 0.25\n",
    "\n",
    "All these gave marginal improvements but it is was a matter of first 3 places :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
